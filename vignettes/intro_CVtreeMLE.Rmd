---
title: "Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE"
author: "[David McCoy](https://davidmccoy.org)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/vignette-"
)
```


## Introduction

Data adaptive target parameters constitute a flexible framework for 
estimating the effects of a data adaptively determined target parameter. 
There is a literature on the dangers of deriving parameters data-adaptively 
and the common
approach for deriving consistent inference for a data-adaptively defined 
parameter is to use  sample-splitting. In sample-splitting the researcher 
splits the full data into a training set used to define the parameter, 
and an estimation sample in which estimates are derived given the parameter 
identified in training. Of course this approach can be costly when new samples
are collected for the estimation data or loses power when the full data is 
simply split. Our proposed approach for decision trees applied to mixtures,
aims to preserve the data-adaptive part of the sample splitting algorithm 
but we define an average of the data-adaptive parameter estimates across 
estimation samples based on arbitrary splits in  V-fold cross-validation. 
In this way, one can still use the power of the entire dataset.
The `CVtreeMLE` package implements decision tree algorithms for computing 
thresholds in exposures that best explain an outcome while flexibly adjusting
for covariates. The nodes in decision trees are represented as a binary exposure
for which targeted minimum loss-based (TML) estimates are derived for 
the counterfactual mean outcome difference if all individual were exposed 
to the rule compared to if no one was exposed to the mixture.

Two types of results are given for decision trees identified in the mixture 
and marginal space of the exposure. V-fold specific results give the ATE and
variance estimates for a fold specific set of rules. The pooled ATE takes the
average across folds to gain power (reduce variance). We give both so that 
researchers can look to see how consistent rules are given sample splitting 
which adds information to the pooled result. 

For a technical 
presentation, the interested reader is invited to consult @mccoy2022CVtreeMLE 
or the earlier work of @vdl2011targeted, @vdl2018targeted, and 
@vdl2022targeted. For more background on data-adaptive target parameters see 
@Hubbard2016 and chapter 9 in @vdl2018targeted.

To start, let's load the packages we'll need and set a seed for simulation.
We will use a real-world data example with known ground-truth to show the 
functionality of `CVtreeMLE`.

```{r setup}
library(data.table)
library(CVtreeMLE)
library(sl3)
library(kableExtra)
library(dplyr)

seed <- 5454432
set.seed(seed)
```

## Simulate Data

```{r simulate data, warning=FALSE}
NIEHS_data <- NIEHS_data_1

head(NIEHS_data) %>%
  kableExtra::kbl(caption = "NIEHS Data") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```
For detailed information on this simulated data please see: 
https://github.com/niehs-prime/2015-NIEHS-MIxtures-Workshop

Briefly, this synthetic data can be considered as the results of a prospective
cohort epidemiologic study. The outcome cannot cause the exposures (as might
occur in a cross-sectional study). Correlations between exposure variables can
be thought of as caused by common sources or modes of exposure. The nuisance 
variable Z can be assumed to be a potential confounder and not a collider.
There are 7 exposures which have a complicated dependency structure. $X_3$ and
$X_6$ do not have an impact on the outcome. 

One issue is that many machine learning algorithms will fail given only 1 
variable passed as a feature so let's add some other covariates.

```{r add covariates, warning=FALSE}
NIEHS_data$Z2 <- rbinom(nrow(NIEHS_data),
                                        size = 1,
                                        prob = 0.3)

NIEHS_data$Z3 <- rbinom(nrow(NIEHS_data),
                                        size = 1,
                                        prob = 0.1)
```


## Run `CVtreeMLE`


```{r run simulation, warnings = FALSE}
ptm <- proc.time()

NIEHS_results <- CVtreeMLE(data = as.data.frame(NIEHS_data),
                         w = c("Z", "Z2", "Z3"),
                         a = c(paste("X", seq(7), sep = "")),
                         y = "Y",
                         n_folds = 5,
                         seed = seed,
                         parallel_cv = TRUE,
                         family = "gaussian",
                         num_cores = 6,
                         max_iter = 1)
proc.time() - ptm
```

## Mixture Results

We can look at the pooled TMLE results for this model, let's focus on the 
results that are found consistenly across all the folds. By consistent we mean
rules that have the same sets of variables with the same direction of effect 
across all the folds. Weaker interactions may only be found in some folds, here
we will focus on the strongest: 

```{r pooled mixture results}
pooled_mixture_results <- NIEHS_results$`Pooled TMLE Mixture Results`

pooled_mixture_results %>%
  dplyr::filter(Proportion_Folds == 1.0)  %>%
  dplyr::arrange(desc(`Mixture ATE`)) %>%
  kableExtra::kbl(caption = "Pooled TMLE Mixture Results") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```

Above, the pooled mixture ATE for each rule is given. Let's focus on the first 
row that has $X_5*X_7$. The Mixture ATE shows the ATE, or the expected mean 
outcome difference if all individuals were exposed to the mixture rule compared 
to if no individuals were exposed to the mixture rule. This ATE is calculated
by pooling the nuisance parameter values across the validation folds and 
doing a TMLE update on the initial counterfactuals across the full data. 
Standard error, CIs and P-values are derived from the efficient influence
function. Because rules may deviate from fold to fold we create a union
rule. To see this more clearly let's look at the fold specific results for 
this so-called interaction: 

```{r fold specific mixture results}
vfold_mixture_results <- NIEHS_results$`V-Specific Mix Results`

vfold_mixture_results$X5X7 %>%
  kableExtra::kbl(caption = "X5 and X7 Interaction") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```
Above we see the fold specific results for $X_5 * X_7$. Let's focus on $X_5$ in 
the rules listed under _mix_rule_. Here we see that in fold 1 the threshold
for $X_5$ is 1.332, in fold 2 it's 0.374 etc. Of these thresholds we select
the highest because this value covers observations indicated by lower 
thresholds. This is because, in the case of $X_5$ the direction is 
less than or equal to. 

So in our union rule the threshold for $X_5$ is 1.324 because 
this threshold covers all other observations indicated by slightly lower
rules in the other folds. The same is true for $X_7$ but we select the 
lower threshold because the direction is $>=$. This is why in the first table
the union rule is X5 < 1.324 & X7 > 0.242. This rule covers all observations
indicated by the fold specific rules used to generate the ATE. 

Overall, the ATE for this rule is 7.5 for the pooled TMLE estimate. 
We see this is higher than if we were to simply take a weighted average. 

What happens is that the nuisance parameters 
used to generate the v-fold specific results are instead pooled and a TMLE
update is done across all the validation data. This is why the results for 
the pooled TMLE update are different compared to the pooled result shown in 
the v-fold specific table, this is the weighted average across the folds, 
similar to a meta-analysis. 

## Comparing Mixture Rules to Ground-Truth

In terms of toxicology, there are the following kinds of interaction 
(relative to concentration addition) in this data:

* X1 and X2 TEF (toxic equivalent factor), a special case of concentration addition (both increase Y)
* X1 and X4 competitive antagonism (similarly for X2 and X4)
* X1 and X5 competitive antagonism (similarly for X2 and X4)
* X1 and X7 supra-additive (“synergy”) (similarly for X2 and X7)
* X4 and X5 TEF, a type of concentration addition (both decrease y)
* X4 and X7 antagonism (unusual kind) (similarly for X5 and X7)


As we can see, $X_5 * X_7$ is listed as having antagonism. This is 
picked up in `CVtreeMLE` because the thresholds that define the ATE are for 
regions where $X_7$ is high and $X_5$ is low. Thus the impact is greater in 
this region because $X_5$ antagonizes the effect of $X_7$. The ground-truth data
also has interactions for $X_2 * X_7$ which we detect amongst others. Generally,
`CVtreeMLE` identifies thresholds for these interactions.  

## Plotting Mixture Results

We can plot our v-fold mixture results findings using the `plot_mixture_results`
function. This will return a list of plots with names corresponding to the 
interactions found.

```{r plot sim_mixture_results, fig.height = 3, fig.width = 8}
mixture_plots <- plot_mixture_results(
  v_intxn_results = NIEHS_results$`V-Specific Mix Results`,hjust = 0.8)
mixture_plots$X5X7
```

## Marginal Results

`CVtreeMLE` also estimates marginal thresholds for each exposure if the exposure
has explanatory power on the outcome. Here, ensemble decision trees are fit
to each exposure individually while controlling for other mixture components
and covariates. ATEs are given using the lowest region as the reference 
exposure. 

Let's first look at the pooled results: 

```{r marginal results}
pooled_marginal_results <- NIEHS_results$`Pooled TMLE Marginal Results`

pooled_marginal_results %>%
  dplyr::filter(`Proportion in Fold` == 1)  %>%
  kableExtra::kbl(caption = "Pooled Marginal Results") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```

The above table shows thresholds identified with subsequent ATE estimates given
those thresholds for each variable where the regions were found in 100% of the 
folds. For example, for $X_1$ one threshold value was found consistently, we 
compare the expected outcome above this threshold to below. The marginal
rule is what we compare to the reference level. Reference rules can be found 
here: 

```{r marginal refs}
pooled_marginal_refs<- NIEHS_results$`Pooled Marginal Refs`

pooled_marginal_refs %>%
  kableExtra::kbl(caption = "Pooled Marginal Reference Regions") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```
Let's examine this a bit more in detail by looking at the results for just
one variable. 

```{r marginal_results_example}
pooled_marginal_results %>%
  dplyr::filter(`Proportion in Fold` == 1)  %>%
  dplyr::filter(var == "X1")  %>%
  kableExtra::kbl(caption = "Pooled Marginal Results") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```
Above, this means that, in the leaves of the trees generated for $X_1$ at 
least one partition was found in 100% of the folds. This region is 
$X_1$ > 0.32 & $X_1$ < 4.7. The outcome in this region is compared to the 
reference region found across all the folds. Which is: 

```{r marginal refs example}
pooled_marginal_refs %>%
  dplyr::filter(`Variable Quantile` == "X1_1") %>%
  kableExtra::kbl(caption = "Pooled Marginal Reference Regions X1") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```

We see there is some overlap in these regions and that's to be expected given
variation across the folds. In this example we can say that, levels in $X_1$ 
change somewhere between 0.3 and 2. For this reason it's more interpretable
to look at the v-fold specific results. 

```{r fold marginal results}
fold_marginal_results<- NIEHS_results$`V-Specific Marg Results`

fold_marginal_results$`X1_2-X1_1` %>%
  kableExtra::kbl(caption = "V-fold specific results for X1 regions 1 and 2") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```
This gives a better piture of what's going on. Above we see the fold specific
ATEs for comparing the region above the reference region for $X_1$. 

## Runtime Performance Guidelines

`CVtreeMLE` uses ensemble machine learning which is obviously computationally
demanding. The `utils_create_sls.R` function creates some lean yet 
non-parametric ensemble learners for each parameter. For example, glm, 
elastic net, random forest, and xgboost models are created for the 
nuisance parameters and decision trees of various depths are created for 
the decision tree fitting Super Learner. Users are also welcome to pass their
own stacks of learners in to `CVtreeMLE` if they feel the default estimators
are insufficient givent the complexity of the data. Additionally, to help
with computational time, `CVtreeMLE` uses the future package for sequential 
and parallel processing. The default functionality is to parallelize
across the folds (parallel_cv = TRUE). The default parallelization type is 
multi-session, this is because we expect most users to be programming in 
Rstudio where multicore is not supported. Thus, a user should expected 
multiple cores to be in use, the number which is equal to num_cores with 
high CPU usage on each core because the data and models are copied to 
each core. This is different from multicore where the data is not copied. 



