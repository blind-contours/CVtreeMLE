---
title: "Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE"
author: "[David McCoy](https://davidmccoy.org)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Introduction

Consider we observe $n$ i.i.d. copies of a random variable with a probability distribution that is known to be an element of a particular statistical model. In order to define our statistical target we use cross-validation and partition the total sample into $V$ equal size sub-samples, and use this partitioning to define $V$ split estimation samples (one of the V subsamples) and corresponding complementary parameter-generating sample that is used to generate a target parameter. For each of the $V$ parameter-generating samples, we apply a decision tree algorithm that maps a set of continuous mixture variables into a rule which is applied to the estimation sample in order to derive the statistical target parameter of interest, the average treatment effect. That is, the `CVtreeMLE` package implements algorithms to first identify thresholds in the mixture space using decision trees in the parameter generating sample and (given a set of thresholds is identified which satisfy some loss function), does targeted minimum loss-based (TML) estimations of the counterfactual mean had all individual been exposed to levels of exposures used in the rule $(Y | A = 1, W)$ compared to the counterfactual mean had all individuals not been exposed $(Y | A = 0, W)$. 

In the current implementation, we define our sample-split data-adaptive statistical target parameter as the average of these V-sample specific target parameters. However, future implementations will provide fold-specific estimates to improve interpretability of rules found across the folds. `CVtreeMLE` demonstrates the use of a data-adaptive methodology which allows new opportunities for statistical learning from data that go beyond the usual requirement that the estimand is a priori defined in order to allow for proper statistical inference. `CVtreeMLE` provides a rigorous statistical methodology for the application of decision trees to a mixture space that is both exploratory and confirmatory for the analysis of mixtures within the same data. For more background on Targeted Learning, consider consulting @vdl2011targeted, @vdl2018targeted, and @vdl2022targeted. For more background on data-adaptive target parameters see @Hubbard2016 and chapter 9 in @vdl2018targeted. 

## Decision Trees

Consider our outcome $Y$ is generated through the sum of two functions $h(A)$ and $g(W)$ ($Y = h(A) + g(W)$). Here, $h(A)$ is a decision tree algorithm applied to the vector of exposures $A$. Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression and are widely used since they are easy to interpret, handle many types of outcomes, and do not require feature scaling or dimensional reduction. Decision trees are particularly good at capturing non-linearities and feature interactions which is of specific interest in mixed exposures. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks. However, the drawback of these techniques is that the easy interpretability of single classification and regression tree (CART) is lost and the analyst has to rely on measures of variable importance that do not directly link the exposure with the outcome (change in model fit when a variable is removed compared to the full model). 

In most research scenarios, the analyst is also interested in deriving statistical inference after fitting a single CART algorithm, that is, the analyst is interested in a p-value for the difference in mean outcomes between the nodes that delineate certain regions in the joint space. Inference on this difference in means involves conditioning on parent nodes in the branch that led to this split in the tree. There are two main issues with this approach of deriving statistical inference in CART models. The first is that the same data is used to both identify partitioning nodes and make statistical inference on these nodes that were not known a priori. Obviously, this approach leads to biased estimates due to overfitting as one is "double dipping" by using the full data to both identify values in the exposures used as splits in the decision tree and make statistical inference given these split values.  The second issue is the need to flexibly control for covariates $W$ in an unrestricted, non-parametric fashion, while simultaneously fitting a CART model to a vector of exposures $A$. This is because, in the context of mixed exposures, the analyst is interested in interpretable thresholds of chemical exposures with robust statistical inference after flexibly adjusting for covariates (not making additive assumptions and allowing for many types of interecations in the covariate space). 

Altogether, to meet this goal requires a new statistical approach in tree fitting on $A$ that flexibly controls for covariates $W$. We do this using a simple iterative backfitting algorithm. The additive model described is a non-parametric regression model of the form: 

\begin{equation}
Y_i =  h(A_{i}) + g(W_{i}) +  \epsilon_i
\end{equation}

Where $A_i$ is a vector of exposure values for individual $i$ and $W_i$ is a vector of covariate values for individual $i$. The backfitting procedure is done in the following way: 

* **Initialize** $g(W)$ by fitting a discrete Super Learner for $Y|W$ 
  + **Predict** $Y|W$ to get $Y^*$ 
* **Initialize** $h(A)$ by fitting a model-based recursive partitioning algorithm based on generalized linear models for $Y|A$ 
  + **Predict** $Y|A$ to get $Y^{**}$ 

* **Do** until convergence: 
  + *Fit* $g(W)$ offset by $Y^{**}$
  + *Predict* $Y$ using $g(W)$ with no offset to get new $Y^*$
  + *Predict* $Y$ using $g(W)$ with offset ($g(A,W)$)
  + *Fit* $h(A)$ offset by $Y^{*}$
  + *Predict* $Y$ using $h(A)$ with no offset to get new $Y^{**}$ 
  + *Predict* $Y$ using $h(A)$ with offset ($h(A,W)$)
  
_Where_ convergence is defined as no change in the difference between $g(A,W)$ and $h(A,W)$ between iterations.  
  



To start, let's load the packages we'll need and set a seed for simulation:

```{r setup}
library(data.table)
library(sl3)
library(CVtreeMLE)

set.seed(11249)
```

