% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_iterative_marg_rule_backfitting.R
\name{fit_iterative_marg_rule_backfitting}
\alias{fit_iterative_marg_rule_backfitting}
\title{Iteratively Backfit a Super Learner, h(x) = Y|M_neX,W, and Indiviidual Decision Algorithms, g(x), for each Y|M_x until Convergence.}
\usage{
fit_iterative_marg_rule_backfitting(mix_comps, At, W, Q1_stack, fold, verbose)
}
\arguments{
\item{mix_comps}{A vector of characters indicating variables for the mixture components}

\item{At}{Training data}

\item{W}{A vector of characters indicating variables that are covariates#'}

\item{Q1_stack}{Stack of algorithms made in SL 3 used in ensemble machine learning to fit Y|W}

\item{fold}{Current fold in the cross-validation}

\item{verbose}{Run in verbose setting}
}
\value{
Rules object. TODO: add more detail here.
}
\description{
Iteratively Backfit a Super Learner, h(x) = Y|M_neX,W, and Indiviidual Decision Algorithms, g(x), for each Y|M_x until Convergence.
}
\details{
Same procedure as \code{fit_iterative_mix_rule_backfitting} but the Super Learner controls for W and other mixture variables not equal to the mixture variable of interest, \code{glmtree} is used to find partitions
instead of \code{pre}.
}
