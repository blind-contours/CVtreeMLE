  @Article{partykit2015,
    title = {{partykit}: A Modular Toolkit for Recursive Partytioning
      in {R}},
    author = {Torsten Hothorn and Achim Zeileis},
    journal = {Journal of Machine Learning Research},
    year = {2015},
    volume = {16},
    pages = {3905-3909},
    url = {https://jmlr.org/papers/v16/hothorn15a.html},
  }
  @Article{Hothorn2006,
    title = {Unbiased Recursive Partitioning: A Conditional Inference
      Framework},
    author = {Torsten Hothorn and Kurt Hornik and Achim Zeileis},
    journal = {Journal of Computational and Graphical Statistics},
    year = {2006},
    volume = {15},
    number = {3},
    doi = {10.1198/106186006X133933},
    pages = {651--674},
  }
  @Article{Zeileis2008,
    title = {Model-Based Recursive Partitioning},
    author = {Achim Zeileis and Torsten Hothorn and Kurt Hornik},
    journal = {Journal of Computational and Graphical Statistics},
    year = {2008},
    volume = {17},
    number = {2},
    doi = {10.1198/106186008X319331},
    pages = {492--514},
  }

@article{Fokkema2020a,
abstract = {Prediction rule ensembles (PREs) are sparse collections of rules, offering highly inter-pretable regression and classification models. This paper shows how they can be fitted using function pre from R package pre, which derives PREs largely through the method-ology of Friedman and Popescu (2008). The implementation and functionality of pre is described and illustrated through application on a dataset on the prediction of depres-sion. Furthermore, accuracy and sparsity of pre is compared with that of single trees, random forests, lasso regression and the original RuleFit implementation of Friedman and Popescu (2008) in four benchmark datasets. Results indicate that pre derives ensembles with predictive accuracy similar to that of random forests, while using a smaller number of variables for prediction. Furthermore, pre provided better accuracy and sparsity than the original RuleFit implementation.},
archivePrefix = {arXiv},
arxivId = {1707.07149},
author = {Fokkema, Marjolein},
doi = {10.18637/jss.v092.i12},
eprint = {1707.07149},
file = {:Users/davidmccoy/Downloads/v92i12.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Decision trees,Ensemble learning,Lasso penalty,Prediction rules,R},
number = {12},
title = {{Fitting prediction rule ensembles with R package pre}},
volume = {92},
year = {2020}
}

@incollection{diaz2018stochastic,
  title={Stochastic Treatment Regimes},
  author={D{\'\i}az, Iv{\'a}n and {van der Laan}, Mark J},
  booktitle={Targeted Learning in Data Science: Causal Inference for Complex
    Longitudinal Studies},
  pages={167--180},
  year={2018},
  publisher={Springer Science \& Business Media}
}

@article{diaz2012population,
  title={Population intervention causal effects based on stochastic
    interventions},
  author={D{\'\i}az, Iv{\'a}n and {van der Laan}, Mark J},
  journal={Biometrics},
  volume={68},
  number={2},
  pages={541--549},
  year={2012},
  publisher={Wiley Online Library}
}

@book{vdl2018targeted,
  title={Targeted Learning in Data Science: Causal Inference for Complex
    Longitudinal Studies},
  author={{van der Laan}, Mark J and Rose, Sherri},
  year={2018},
  publisher={Springer Science \& Business Media}
}

@book{vdl2011targeted,
  title={Targeted learning: causal inference for observational and experimental
         data},
  author={{van der Laan}, Mark J and Rose, Sherri},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{rose2011targeted2sd,
  title={A targeted maximum likelihood estimator for two-stage designs},
  author={Rose, Sherri and {van der Laan}, Mark J},
  journal={The International Journal of Biostatistics},
  volume={7},
  number={1},
  pages={1--21},
  year={2011}
}

@article{diaz2011super,
  title={Super learner based conditional density estimation with application to
    marginal structural models},
  author={D{\'\i}az, Iv{\'a}n and {van der Laan}, Mark J},
  journal={The International Journal of Biostatistics},
  volume={7},
  number={1},
  pages={1--20},
  year={2011},
  publisher={De Gruyter}
}

@article{diaz2020causal,
  title={Causal mediation analysis for stochastic interventions},
  author={D{\'\i}az, Iv{\'a}n and Hejazi, Nima S},
  year={2020},
  url = {https://doi.org/10.1111/rssb.12362},
  doi = {10.1111/rssb.12362},
  journal={Journal of the Royal Statistical Society: Series B (Statistical
    Methodology)},
  volume = {82},
  number = {3},
  pages = {661-683},
  publisher={Wiley Online Library}
}

@article{hejazi2020efficient,
  title = {Efficient nonparametric inference on the effects of stochastic
    interventions under two-phase sampling, with applications to vaccine
    efficacy trials},
  author = {Hejazi, Nima S and {van der Laan}, Mark J and Janes, Holly E and
    Gilbert, Peter B and Benkeser, David C},
  year={2020},
  journal = {Biometrics},
  publisher = {Wiley Online Library},
  volume={},
  number={},
  pages={},
  url = {https://doi.org/10.1111/biom.13375},
  doi = {10.1111/biom.13375}
}

@article{pfanzagl1985contributions,
  title={Contributions to a general asymptotic statistical theory},
  author={Pfanzagl, J and Wefelmeyer, W},
  journal={Statistics \& Risk Modeling},
  volume={3},
  number={3-4},
  pages={379--388},
  year={1985}
}

@article{vdl2007super,
  title={Super learner},
  author={{van der Laan}, Mark J and Polley, Eric C and Hubbard, Alan E},
  journal={Statistical applications in genetics and molecular biology},
  volume={6},
  number={1},
  year={2007}
}

@book{pearl2000causality,
  title={Causality},
  author={Pearl, Judea},
  year={2000},
  publisher={Cambridge university press}
}

@software{hejazi2021haldensify,
  author = {Hejazi, Nima S and Benkeser, David C and {van der Laan}, Mark J},
  title = {{haldensify}: Highly adaptive lasso conditional density estimation},
  year  = {2021},
  note = {{R} package version 0.2.1},
  url = {https://github.com/nhejazi/haldensify/},
  doi = {10.5281/zenodo.3698329}
}

@software{coyle2021sl3,
  author = {Coyle, Jeremy R and Hejazi, Nima S and Malenica, Ivana and
    Phillips, Rachael V and Sofrygin, Oleg},
  title = {{sl3}: Modern Pipelines for Machine Learning and {Super Learning}},
  year = {2021},
  note = {{R} package version 1.4.2},
  url = {https://github.com/tlverse/sl3/},
  doi = {10.5281/zenodo.1342293}
}

@software{coyle2021hal9001,
  author = {Coyle, Jeremy R and Hejazi, Nima S and Phillips, Rachael V and {van
    der Laan}, Lars W and {van der Laan}, Mark J},
  title = {{hal9001}: The scalable highly adaptive lasso},
  year  = {2021},
  note = {{R} package version 0.4.1},
  url = {https://github.com/tlverse/hal9001/},
  doi = {10.5281/zenodo.3558313}
}

@book{vdl2022targeted,
  doi = {},
  url = {https://tlverse.org/tlverse-handbook/},
  year = {2022},
  publisher = {CRC Press},
  author = {{van der Laan}, Mark J and Coyle, Jeremy R and Hejazi, Nima S and
    Malenica, Ivana and Phillips, Rachael V and Hubbard, Alan E},
  title = {{Targeted Learning in \texttt{R}: A Causal Data Science Handbook}},
  note = {in preparation},
  keywords = {books}
}

@article{vdl2004asymptotic,
  title={Asymptotic optimality of likelihood-based cross-validation},
  author={{van der Laan}, Mark J and Dudoit, Sandrine and Keles, Sunduz},
  journal={Statistical Applications in Genetics and Molecular Biology},
  volume={3},
  number={1},
  pages={1--23},
  year={2004}
}

@article{vdl2006cross,
  title={The cross-validated adaptive epsilon-net estimator},
  author={{van der Laan}, Mark J and Dudoit, Sandrine and {van der Vaart}, Aad
    W},
  journal={Statistics \& Decisions},
  volume={24},
  number={3},
  pages={373--395},
  year={2006},
  publisher={Oldenbourg Wissenschaftsverlag}
}

@article{vdv2006oracle,
  title={Oracle inequalities for multi-fold cross validation},
  author={{van der Vaart}, Aad W and Dudoit, Sandrine and {van der Laan}, Mark
    J},
  journal={Statistics \& Decisions},
  volume={24},
  number={3},
  pages={351--371},
  year={2006},
  publisher={Oldenbourg Wissenschaftsverlag}
}

@article{dudoit2005asymptotics,
  title={Asymptotics of cross-validated risk estimation in estimator selection
    and performance assessment},
  author={Dudoit, Sandrine and {van der Laan}, Mark J},
  journal={Statistical Methodology},
  volume={2},
  number={2},
  pages={131--154},
  year={2005},
  publisher={Elsevier}
}

@article{Hubbard2016,
abstract = {Consider one observes n i.i.d. copies of a random variable with a probability distribution that is known to be an element of a particular statistical model. In order to define our statistical target we partition the sample in V equal size sub-samples, and use this partitioning to define V splits in an estimation sample (one of the V subsamples) and corresponding complementary parameter-generating sample. For each of the V parameter-generating samples, we apply an algorithm that maps the sample to a statistical target parameter. We define our sample-split data adaptive statistical target parameter as the average of these V-sample specific target parameters. We present an estimator (and corresponding central limit theorem) of this type of data adaptive target parameter. This general methodology for generating data adaptive target parameters is demonstrated with a number of practical examples that highlight new opportunities for statistical learning from data. This new framework provides a rigorous statistical methodology for both exploratory and confirmatory analysis within the same data. Given that more research is becoming "data-driven", the theory developed within this paper provides a new impetus for a greater involvement of statistical inference into problems that are being increasingly addressed by clever, yet ad hoc pattern finding methods. To suggest such potential, and to verify the predictions of the theory, extensive simulation studies, along with a data analysis based on adaptively determined intervention rules are shown and give insight into how to structure such an approach. The results show that the data adaptive target parameter approach provides a general framework and resulting methodology for data-driven science.},
author = {Hubbard, Alan E. and Kherad-Pajouh, Sara and {Van Der Laan}, Mark J.},
doi = {10.1515/ijb-2015-0013},
file = {:Users/davidmccoy/Library/Application Support/Mendeley Desktop/Downloaded/Hubbard, Kherad-Pajouh, Van Der Laan - 2016 - Statistical Inference for Data Adaptive Target Parameters.pdf:pdf},
issn = {15574679},
journal = {International Journal of Biostatistics},
keywords = {Asymptotic linearity,clustering,cross-validation,data mining,influence curve,loss-function,machine learning,risk,sample splitting,sub-group analysis,super-learner,targeted maximum likelihood estimation},
number = {1},
pages = {3--19},
pmid = {27227715},
title = {{Statistical Inference for Data Adaptive Target Parameters}},
volume = {12},
year = {2016}
}

@article{Zheng2010,
abstract = {We consider a targeted maximum likelihood estimator of a path-wise differen- tiable parameter of the data generating distribution in a semi-parametric model based on observing n independent and identically distributed observations. The targeted maximum likelihood estimator (TMLE) uses V-fold sample splitting for the initial estimator in order to make the TMLE maximally robust in its bias re- duction step. We prove a general theorem that states asymptotic efficiency (and thereby regularity) of the targeted maximum likelihood estimator when the initial estimator is consistent and a second order term converges to zero in probability at a rate faster than the square root of the sample size, but no other meaningful conditions are needed. In particular, the conditions of this theorem allow the full utilization of loss based super learning to obtain the initial estimator. In particular, the theorem proves that first order efficient and unbiased estima- tion is enhanced in an important way by using adaptive estimators such as an super learner, thereby formally dealing with the concern that adaptive estimation might make it harder to construct valid confidence intervals. On the contrary, the theorem teaches us that to achieve first order efficiency and regularity, it is crucial to estimate the relevant parts of the true data generating distribution as good as possible. The theorem is applied to prove asymptotic efficiency of the targeted maximum likelihood estimator of the additive causal effect of a binary treatment on an outcome in a randomized controlled trial and in an observational study. Excellent finite sample performance of this estimator has been demonstrated in past articles (e.g.van der Laan et al. (September, 2009), Gruber and van der Laan (2010), Stitelman and van der Laan (2010), Petersen et al. (2010).},
author = {Zheng, Wenjing and van der Laan, MJ},
file = {:Users/davidmccoy/Library/Application Support/Mendeley Desktop/Downloaded/Zheng, Laan - 2010 - Asymptotic theory for cross-validated targeted maximum likelihood estimation.pdf:pdf},
journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
keywords = {Asymptotic efficiency,asymptotic linearity,canonical gradient,cross-validation,empirical process theory,targeted maximum likelihood estimator},
number = {273},
title = {{Asymptotic theory for cross-validated targeted maximum likelihood estimation}},
url = {http://biostats.bepress.com/ucbbiostat/paper273/},
year = {2010}
}