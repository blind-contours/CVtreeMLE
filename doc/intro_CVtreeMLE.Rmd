---
title: "Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE"
author: "[David McCoy](https://www.davidmccoy.org/)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8} 
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/vignette_"
)
```


## Introduction

Data adaptive target parameters constitute a flexible framework for 
estimating the effects of a data adaptively determined target parameter. 
There is a literature on the dangers of deriving parameters data-adaptively 
and the common
approach for deriving consistent inference for a data-adaptively defined 
parameter is to use  sample-splitting. In sample-splitting the researcher 
splits the full data into a training set used to define the parameter, 
and an estimation sample in which estimates are derived given the parameter 
identified in training. Of course this approach can be costly when new samples
are collected for the estimation data or loses power when the full data is 
simply split. Our proposed approach for a decision tree applied to mixtures,
aims to preserve the data-adaptive part of the sample splitting algorithm 
but we define an average of the data-adaptive parameter estimates across 
estimation samples based on arbitrary splits in K-fold cross-validation. 
In this way, one can still use the power of the entire dataset.
The `CVtreeMLE` package implements decision tree algorithms for computing 
thresholds in exposures that maximize or minimize the outcome while flexibly adjusting
for covariates. The nodes in the decision tree are represented as a binary 
exposures
for which targeted minimum loss-based estimates (TMLE)  are derived for 
the counterfactual mean outcome difference if all individual were exposed 
to the rule compared to the observed outcome under observed exposure distribution. We call our estimate the ARE, the average regional exposure effect. 

Two types of results are given for the region found to minimize or maximize the expected outcome. K-fold specific results for a given ARE and
variance estimates for a fold specific region. The pooled ARE takes the
average across folds to gain power (reduce variance). The pooled ARE estimates the oracle target parameter which is the region that maximizes or minimizes the expected outcome. This of course can change across the folds and therefore this estimate, if there is inconcistency in the region estiamted, can lead to an amalgamation of different regions. However, if there is strong signal for a particular region, the same exposures will be used in the region and therefore the pooled parameter gains power, leveraging the k-fold specific estimates. 

We give both so that 
researchers can look to see how consistent rules are given sample splitting 
which adds information to the pooled result. 

For a technical 
presentation, the interested reader is invited to consult @mccoy2022CVtreeMLE 
or the earlier work of @vdl2011targeted, @vdl2018targeted, and 
@vdl2022targeted. For more background on data-adaptive target parameters see 
@Hubbard2016 and chapter 9 in @vdl2018targeted.

Briefly, what this package will do is first, given the provided data it will partition 
the data into folds based on the n_folds parameter provided. In the training fold, 
we apply a custom decision tree that greedily partitions the exposure space, at each partition
it uses g-computation to estimate the expected outcome for that region controlling for covariates. 
It finds the region that maximizes or minimizes the expected outcome. This is done for each fold. 
Then for each region that maximizes or minimizes the expected outcome in each fold, we estimate the effect comparing the expected outcome if everyone was forced to be in that region compared to the observed outcome under observed exposure. This is the ARE. This is done for each fold and we also pool across the folds. This procedure is done with CV-TMLE. The output are thresholds and the accompanying ARE for the region that has the maximimum or minimum expected outcome. 

To start, let's load the packages we'll need and set a seed for simulation.
We will use a real-world data example with known ground-truth to show the 
functionality of `CVtreeMLE`.

```{r setup, message=FALSE, warning=FALSE}
library(data.table)
library(CVtreeMLE)
library(sl3)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(purrr)

seed <- 5454433
set.seed(seed)
```

## National Institute of Environmental Health Data

We will use simulated data from the 2015 NIEHS Mixtures Workshop which was 
developed to determine if new mixture methods detect ground-truth interactions
built into the simulated data. In this way we can simultaneously show
`CVtreeMLE` output, interpretation and validity. 


```{r load_NIEHS_data, warning=FALSE}
niehs_data <- NIEHS_data_1

head(niehs_data) %>%
  kableExtra::kbl(caption = "NIEHS Data") %>%
  kableExtra::kable_classic(full_width = FALSE, html_font = "Cambria")
```
For detailed information on this simulated data please see: 


https://github.com/niehs-prime/2015-NIEHS-MIxtures-Workshop

Briefly, this synthetic data can be considered the results of a prospective
cohort epidemiologic study. The outcome cannot cause the exposures (as might
occur in a cross-sectional study). Correlations between exposure variables can
be thought of as caused by common sources or modes of exposure. The nuisance 
variable Z can be assumed to be a potential confounder and not a collider.
There are 7 exposures which have a complicated dependency structure. $X_3$ and
$X_6$ do not have an impact on the outcome. 

One issue is that many machine learning algorithms will fail given only 1 
variable passed as a feature so let's add some other covariates.

```{r add_covariates, warning=FALSE}
niehs_data$Z2 <- rbinom(nrow(niehs_data),
  size = 1,
  prob = 0.3
)

niehs_data$Z3 <- rbinom(nrow(niehs_data),
  size = 1,
  prob = 0.1
)
```


## Run `CVtreeMLE`


```{r run_simulation, warnings = FALSE}
ptm <- proc.time()

niehs_results <- CVtreeMLE(
  data = as.data.frame(niehs_data),
  w = c("Z", "Z2", "Z3"),
  a = c(paste("X", seq(7), sep = "")),
  y = "Y",
  n_folds = 5,
  seed = seed,
  parallel_cv = TRUE,
  parallel = TRUE,
  family = "continuous",
  num_cores = 8,
  min_max = "min",
  min_obs = 25
)
proc.time() - ptm
```

## Mixture Results

First let's look at the oracle estimate. This is the pooled CV-TMLE estimates for our parameter which is the region that maximizes or minimizes the expected outcome - again this could be different if there is incosistencies in estimates across the folds. 

```{r pooled_mixture_results}
pooled_mixture_results <- niehs_results$`Oracle Region Results`

pooled_mixture_results %>%
  kableExtra::kbl(caption = "Oracle Mixture Results") %>%
  kableExtra::kable_classic(full_width = FALSE, html_font = "Cambria")
```

This shows that the estimate for the pooled region that minimizes the expected outcome is -3.77, which is not significant. 


We can look at what regions comprise this oracle region: 

```{r region_specific_pooling}
region_specific_pooling <- niehs_results$`Pooled TMLE Mixture Results`

region_specific_pooling %>%
  kableExtra::kbl(caption = "Region Specific Mixture Results") %>%
  kableExtra::kable_classic(full_width = FALSE, html_font = "Cambria")
```
This shows 80\% of the folds find a region for X2 and 20\% for X1, this becomes more consistent if higher folds are used. 


```{r k_fold_results}
k_fold_results <- niehs_results$`V-Specific Mix Results`

k_fold_results %>%
  kableExtra::kbl(caption = "K-fold Results") %>%
  kableExtra::kable_classic(full_width = FALSE, html_font = "Cambria")
```

## Estimate Stability

Because we are identifying thresholds mixture space
data-adaptively using the training data as we rotate through the folds, 
`CVtreeMLE` will give, in the event that a signal actually exists, more 
consistent results when higher n_fold values are used. Thus, we recommend using
10-fold CV when possible for more consistent and interpretable results. Here, 
5-fold was used simply for convenience in compiling and testing. 

It should be noted that, especially in the marginal results, if the analyst
chooses to report on results for one fold, which does have valid inference, 
the variability of the thresholds and estimates across the folds should 
also be provided for full transparency. 


## Runtime Performance Guidelines

`CVtreeMLE` uses ensemble machine learning which is obviously computationally
demanding. The `utils_create_sls.R` function creates some lean yet 
non-parametric ensemble learners for each parameter. For example, glm, 
elastic net, random forest, and xgboost models are created for the 
nuisance parameters and decision trees of various depths are created for 
the decision tree fitting Super Learner. Users are also welcome to pass their
own stacks of learners in to `CVtreeMLE` if they feel the default estimators
are insufficient given the complexity of the data. Additionally, to help
with computational time, `CVtreeMLE` uses the future package for sequential 
and parallel processing. The default functionality is to parallelize
across the folds (parallel_cv = TRUE). The default parallelization type is 
multi-session, this is because we expect most users to be programming in 
Rstudio where multicore is not supported. Thus, a user should expected 
multiple cores to be in use, the number which is equal to num_cores with 
high CPU usage on each core because the data and models are copied to 
each core. This is different from multicore where the data is not copied. 
As we saw above, in this example, a dataset with 500 observations and 7 
exposures took about 5 minutes with 5 folds. Thus, when using 10-fold CV, 
which we recommend, on a standard environmental epidemiology data set of 500-
1000 observations, `CVtreeMLE` should run easily on a modern local 
machine. 

## References


