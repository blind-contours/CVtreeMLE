<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="David McCoy" />

<meta name="date" content="2022-02-04" />

<title>Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>
<script src="data:application/javascript;base64,JChkb2N1bWVudCkucmVhZHkoZnVuY3Rpb24oKXsKICAgIGlmICh0eXBlb2YgJCgnW2RhdGEtdG9nZ2xlPSJ0b29sdGlwIl0nKS50b29sdGlwID09PSAnZnVuY3Rpb24nKSB7CiAgICAgICAgJCgnW2RhdGEtdG9nZ2xlPSJ0b29sdGlwIl0nKS50b29sdGlwKCk7CiAgICB9CiAgICBpZiAoJCgnW2RhdGEtdG9nZ2xlPSJwb3BvdmVyIl0nKS5wb3BvdmVyID09PSAnZnVuY3Rpb24nKSB7CiAgICAgICAgJCgnW2RhdGEtdG9nZ2xlPSJwb3BvdmVyIl0nKS5wb3BvdmVyKCk7CiAgICB9Cn0pOwo="></script>
<link href="data:text/css,%0A%2Elightable%2Dminimal%20%7B%0Aborder%2Dcollapse%3A%20separate%3B%0Aborder%2Dspacing%3A%2016px%201px%3B%0Awidth%3A%20100%25%3B%0Amargin%2Dbottom%3A%2010px%3B%0A%7D%0A%2Elightable%2Dminimal%20td%20%7B%0Amargin%2Dleft%3A%205px%3B%0Amargin%2Dright%3A%205px%3B%0A%7D%0A%2Elightable%2Dminimal%20th%20%7B%0Amargin%2Dleft%3A%205px%3B%0Amargin%2Dright%3A%205px%3B%0A%7D%0A%2Elightable%2Dminimal%20thead%20tr%3Alast%2Dchild%20th%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%2300000050%3B%0Aempty%2Dcells%3A%20hide%3B%0A%7D%0A%2Elightable%2Dminimal%20tbody%20tr%3Afirst%2Dchild%20td%20%7B%0Apadding%2Dtop%3A%200%2E5em%3B%0A%7D%0A%2Elightable%2Dminimal%2Elightable%2Dhover%20tbody%20tr%3Ahover%20%7B%0Abackground%2Dcolor%3A%20%23f5f5f5%3B%0A%7D%0A%2Elightable%2Dminimal%2Elightable%2Dstriped%20tbody%20tr%3Anth%2Dchild%28even%29%20%7B%0Abackground%2Dcolor%3A%20%23f5f5f5%3B%0A%7D%0A%2Elightable%2Dclassic%20%7B%0Aborder%2Dtop%3A%200%2E16em%20solid%20%23111111%3B%0Aborder%2Dbottom%3A%200%2E16em%20solid%20%23111111%3B%0Awidth%3A%20100%25%3B%0Amargin%2Dbottom%3A%2010px%3B%0Amargin%3A%2010px%205px%3B%0A%7D%0A%2Elightable%2Dclassic%20tfoot%20tr%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A%2Elightable%2Dclassic%20tfoot%20tr%3Afirst%2Dchild%20td%20%7B%0Aborder%2Dtop%3A%200%2E14em%20solid%20%23111111%3B%0A%7D%0A%2Elightable%2Dclassic%20caption%20%7B%0Acolor%3A%20%23222222%3B%0A%7D%0A%2Elightable%2Dclassic%20td%20%7B%0Apadding%2Dleft%3A%205px%3B%0Apadding%2Dright%3A%205px%3B%0Acolor%3A%20%23222222%3B%0A%7D%0A%2Elightable%2Dclassic%20th%20%7B%0Apadding%2Dleft%3A%205px%3B%0Apadding%2Dright%3A%205px%3B%0Afont%2Dweight%3A%20normal%3B%0Acolor%3A%20%23222222%3B%0A%7D%0A%2Elightable%2Dclassic%20thead%20tr%3Alast%2Dchild%20th%20%7B%0Aborder%2Dbottom%3A%200%2E10em%20solid%20%23111111%3B%0A%7D%0A%2Elightable%2Dclassic%2Elightable%2Dhover%20tbody%20tr%3Ahover%20%7B%0Abackground%2Dcolor%3A%20%23F9EEC1%3B%0A%7D%0A%2Elightable%2Dclassic%2Elightable%2Dstriped%20tbody%20tr%3Anth%2Dchild%28even%29%20%7B%0Abackground%2Dcolor%3A%20%23f5f5f5%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20%7B%0Aborder%2Dtop%3A%203px%20double%20%23111111%3B%0Aborder%2Dbottom%3A%203px%20double%20%23111111%3B%0Awidth%3A%20100%25%3B%0Amargin%2Dbottom%3A%2010px%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20tfoot%20tr%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20tfoot%20tr%3Afirst%2Dchild%20td%20%7B%0Aborder%2Dtop%3A%203px%20double%20%23111111%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20caption%20%7B%0Acolor%3A%20%23222222%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20td%20%7B%0Apadding%2Dleft%3A%205px%3B%0Apadding%2Dright%3A%205px%3B%0Acolor%3A%20%23222222%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20th%20%7B%0Apadding%2Dleft%3A%205px%3B%0Apadding%2Dright%3A%205px%3B%0Afont%2Dweight%3A%20normal%3B%0Acolor%3A%20%23222222%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20tbody%20tr%3Alast%2Dchild%20td%20%7B%0Aborder%2Dbottom%3A%203px%20double%20%23111111%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%20thead%20tr%3Alast%2Dchild%20th%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23111111%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%2Elightable%2Dhover%20tbody%20tr%3Ahover%20%7B%0Abackground%2Dcolor%3A%20%23F9EEC1%3B%0A%7D%0A%2Elightable%2Dclassic%2D2%2Elightable%2Dstriped%20tbody%20tr%3Anth%2Dchild%28even%29%20%7B%0Abackground%2Dcolor%3A%20%23f5f5f5%3B%0A%7D%0A%2Elightable%2Dmaterial%20%7B%0Amin%2Dwidth%3A%20100%25%3B%0Awhite%2Dspace%3A%20nowrap%3B%0Atable%2Dlayout%3A%20fixed%3B%0Afont%2Dfamily%3A%20Roboto%2C%20sans%2Dserif%3B%0Aborder%3A%201px%20solid%20%23EEE%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0Amargin%2Dbottom%3A%2010px%3B%0A%7D%0A%2Elightable%2Dmaterial%20tfoot%20tr%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A%2Elightable%2Dmaterial%20tfoot%20tr%3Afirst%2Dchild%20td%20%7B%0Aborder%2Dtop%3A%201px%20solid%20%23EEE%3B%0A%7D%0A%2Elightable%2Dmaterial%20th%20%7B%0Aheight%3A%2056px%3B%0Apadding%2Dleft%3A%2016px%3B%0Apadding%2Dright%3A%2016px%3B%0A%7D%0A%2Elightable%2Dmaterial%20td%20%7B%0Aheight%3A%2052px%3B%0Apadding%2Dleft%3A%2016px%3B%0Apadding%2Dright%3A%2016px%3B%0Aborder%2Dtop%3A%201px%20solid%20%23eeeeee%3B%0A%7D%0A%2Elightable%2Dmaterial%2Elightable%2Dhover%20tbody%20tr%3Ahover%20%7B%0Abackground%2Dcolor%3A%20%23f5f5f5%3B%0A%7D%0A%2Elightable%2Dmaterial%2Elightable%2Dstriped%20tbody%20tr%3Anth%2Dchild%28even%29%20%7B%0Abackground%2Dcolor%3A%20%23f5f5f5%3B%0A%7D%0A%2Elightable%2Dmaterial%2Elightable%2Dstriped%20tbody%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A%2Elightable%2Dmaterial%2Elightable%2Dstriped%20thead%20tr%3Alast%2Dchild%20th%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ddd%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%20%7B%0Amin%2Dwidth%3A%20100%25%3B%0Awhite%2Dspace%3A%20nowrap%3B%0Atable%2Dlayout%3A%20fixed%3B%0Afont%2Dfamily%3A%20Roboto%2C%20sans%2Dserif%3B%0Aborder%3A%201px%20solid%20%23FFFFFF12%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0Amargin%2Dbottom%3A%2010px%3B%0Abackground%2Dcolor%3A%20%23363640%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%20tfoot%20tr%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%20tfoot%20tr%3Afirst%2Dchild%20td%20%7B%0Aborder%2Dtop%3A%201px%20solid%20%23FFFFFF12%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%20th%20%7B%0Aheight%3A%2056px%3B%0Apadding%2Dleft%3A%2016px%3B%0Apadding%2Dright%3A%2016px%3B%0Acolor%3A%20%23FFFFFF60%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%20td%20%7B%0Aheight%3A%2052px%3B%0Apadding%2Dleft%3A%2016px%3B%0Apadding%2Dright%3A%2016px%3B%0Acolor%3A%20%23FFFFFF%3B%0Aborder%2Dtop%3A%201px%20solid%20%23FFFFFF12%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%2Elightable%2Dhover%20tbody%20tr%3Ahover%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF12%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%2Elightable%2Dstriped%20tbody%20tr%3Anth%2Dchild%28even%29%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF12%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%2Elightable%2Dstriped%20tbody%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A%2Elightable%2Dmaterial%2Ddark%2Elightable%2Dstriped%20thead%20tr%3Alast%2Dchild%20th%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23FFFFFF12%3B%0A%7D%0A%2Elightable%2Dpaper%20%7B%0Awidth%3A%20100%25%3B%0Amargin%2Dbottom%3A%2010px%3B%0Acolor%3A%20%23444%3B%0A%7D%0A%2Elightable%2Dpaper%20tfoot%20tr%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A%2Elightable%2Dpaper%20tfoot%20tr%3Afirst%2Dchild%20td%20%7B%0Aborder%2Dtop%3A%201px%20solid%20%2300000020%3B%0A%7D%0A%2Elightable%2Dpaper%20thead%20tr%3Alast%2Dchild%20th%20%7B%0Acolor%3A%20%23666%3B%0Avertical%2Dalign%3A%20bottom%3B%0Aborder%2Dbottom%3A%201px%20solid%20%2300000020%3B%0Aline%2Dheight%3A%201%2E15em%3B%0Apadding%3A%2010px%205px%3B%0A%7D%0A%2Elightable%2Dpaper%20td%20%7B%0Avertical%2Dalign%3A%20middle%3B%0Aborder%2Dbottom%3A%201px%20solid%20%2300000010%3B%0Aline%2Dheight%3A%201%2E15em%3B%0Apadding%3A%207px%205px%3B%0A%7D%0A%2Elightable%2Dpaper%2Elightable%2Dhover%20tbody%20tr%3Ahover%20%7B%0Abackground%2Dcolor%3A%20%23F9EEC1%3B%0A%7D%0A%2Elightable%2Dpaper%2Elightable%2Dstriped%20tbody%20tr%3Anth%2Dchild%28even%29%20%7B%0Abackground%2Dcolor%3A%20%2300000008%3B%0A%7D%0A%2Elightable%2Dpaper%2Elightable%2Dstriped%20tbody%20td%20%7B%0Aborder%3A%200%3B%0A%7D%0A" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">Evaluating Causal Effects of Mixed Exposures using Data Adaptive Decision Trees and CV-TMLE</h1>
<h4 class="author"><a href="https://davidmccoy.org">David McCoy</a></h4>
<h4 class="date">2022-02-04</h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Consider we observe <span class="math inline">\(n\)</span> i.i.d. copies of a random variable with a probability distribution that is known to be an element of a particular statistical model <span class="math inline">\(\mathcal{M}\)</span>. Our exposure <span class="math inline">\(A\)</span>, is an unknown combination of exposure levels to many components of <span class="math inline">\(A\)</span>. That is, unlike in many scenarios where <span class="math inline">\(A\)</span> is simply one binary/continuous treatment/exposure variable, <span class="math inline">\(A\)</span> is in fact a vector of exposures. We are interested in, given some <em>a priori</em> specified algorithm, 1. what mixture components of <span class="math inline">\(A\)</span> have explanatory power on the outcome <span class="math inline">\(Y\)</span> and 2. what levels of these exposures are most impactful. Both the most important variables and the most important levels of these mixture variables are unknown and therefore, the statistical target parameter given these variables/levels is also unknown. Thus, we need a methodology that both allows us to identify these variables/levels, and map these findings into a target parameter with valid statistical inference.</p>
<p>In order to define our statistical target we use cross-validation and partition the total sample into <span class="math inline">\(V\)</span> equal size sub-samples, and use this partitioning to define <span class="math inline">\(V\)</span> split estimation samples (one of the V subsamples) and corresponding complementary parameter-generating sample that is used to generate a target parameter. For each of the <span class="math inline">\(V\)</span> parameter-generating samples, we apply a decision tree algorithm that maps a set of continuous mixture variables into a rule which is applied to the estimation sample in order to derive the statistical target parameter of interest, the average treatment effect. That is, the <code>CVtreeMLE</code> package implements algorithms to first identify thresholds in the mixture space using decision trees in the parameter generating sample and (given a set of thresholds is identified which satisfy some loss function), does targeted minimum loss-based estimations (TMLE) of the counterfactual mean had all individuals been exposed to levels of exposures used in the rule <span class="math inline">\((Y | A = 1, W)\)</span> compared to the counterfactual mean had all individuals not been exposed <span class="math inline">\((Y | A = 0, W)\)</span>.</p>
<p>We define our sample-split data-adaptive statistical target parameter as the average of these V-sample specific target parameters. Statistical inference is done in two ways - 1. a meta-analysis approach which calculates a weighted mean of the target parameter for a respective rule across the V-folds paired with pooled variance estimates and a pooled rule which covers all the V-fold rules and 2. a pooled TMLE update step which targets our parameter of interest and calculates the influence curve across all the folds simultaneously.</p>
<p><code>CVtreeMLE</code> demonstrates the use of a data-adaptive methodology which allows new opportunities for statistical learning from data that go beyond the usual requirement that the estimand is <em>a priori</em> defined in order to allow for proper statistical inference. <code>CVtreeMLE</code> provides a rigorous statistical methodology for the application of decision trees to a mixture space that is both exploratory and confirmatory for the analysis of mixtures within the same data. For more background on Targeted Learning, consider consulting <span class="citation">van der Laan and Rose (2011)</span>, <span class="citation">van der Laan and Rose (2018)</span>, and <span class="citation">van der Laan et al. (2022)</span>. For more background on data-adaptive target parameters see <span class="citation">Hubbard, Kherad-Pajouh, and Van Der Laan (2016)</span> and chapter 9 in <span class="citation">van der Laan and Rose (2018)</span>.</p>
</div>
<div id="iterative-backfitting-of-decision-trees" class="section level2">
<h2>Iterative Backfitting of Decision Trees</h2>
<p>Consider our outcome <span class="math inline">\(Y\)</span> is generated through the sum of two functions <span class="math inline">\(f(A)\)</span> and <span class="math inline">\(h(W)\)</span> (<span class="math inline">\(Y = h(A) + g(W)\)</span>). Notice that, here we assume no interactions between <span class="math inline">\(f(A) and h(W)\)</span>, an assumption we will test later. Here, <span class="math inline">\(f(A)\)</span> is an ensemble of decision tree algorithms applied to the vector of exposures <span class="math inline">\(A\)</span>. Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression and are widely used since they are easy to interpret, handle many types of outcomes, and do not require feature scaling or dimensional reduction. Decision trees are particularly good at capturing non-linearities and feature interactions which is of specific interest in mixed exposures.</p>
<p>In most research scenarios, the analyst is also interested in deriving statistical inference after fitting a single decision tree algorithm, that is, the analyst is interested in a p-value for the difference in mean outcomes between the nodes that delineate certain regions in the joint space. Inference on this difference in means has involved conditioning on parent nodes in the branch that led to this split in the tree. There are two main issues with this approach of deriving statistical inference in decision tree models. The first is that the same data is used to both identify partitioning nodes and make statistical inference on these nodes that were not known <em>a priori</em>. Obviously, this approach leads to biased estimates due to overfitting as one is “double dipping” by using the full data to both identify values in the exposures used as splits in the decision tree and make statistical inference given these split values. The second issue is the need to flexibly control for covariates <span class="math inline">\(W\)</span> in an unrestricted, non-parametric fashion, while simultaneously fitting a decision tree model to a vector of exposures <span class="math inline">\(A\)</span>. This is because, in the context of mixed exposures, the analyst is interested in interpretable thresholds of chemical exposures with robust statistical inference after flexibly adjusting for covariates (not making additive assumptions and allowing for many types of interactions in the covariate space).</p>
<p>Altogether, to meet this goal requires a new statistical approach in tree fitting on <span class="math inline">\(A\)</span> that flexibly controls for covariates <span class="math inline">\(W\)</span> and ensures that the target parameter we derive from the tree is asymptotically effiient - we need a methodology that derives causal inference from decision trees. To do this we start with a simple iterative backfitting algorithm. The additive model described is a semi-parametric regression model of the form:</p>
<p><span class="math display">\[\begin{equation}
Y_i =  f(A_{i}) + h(W_{i}) +  \epsilon_i
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(A_i\)</span> is a vector of exposure values for individual <span class="math inline">\(i\)</span> and <span class="math inline">\(W_i\)</span> is a vector of covariate values for individual <span class="math inline">\(i\)</span>. The backfitting procedure is done in the following way:</p>
<ul>
<li><strong>Initialize</strong> <span class="math inline">\(h(W)\)</span> by fitting a discrete Super Learner for <span class="math inline">\(Y|W\)</span>
<ul>
<li><strong>Predict</strong> <span class="math inline">\(Y|W\)</span> to get <span class="math inline">\(Y^*\)</span></li>
</ul></li>
<li><strong>Initialize</strong> <span class="math inline">\(f(A)\)</span> by fitting a discrete Super Learner of model-based recursive partitioning algorithms for <span class="math inline">\(Y|A\)</span>
<ul>
<li><strong>Predict</strong> <span class="math inline">\(Y|A\)</span> to get <span class="math inline">\(Y^{**}\)</span></li>
</ul></li>
<li><strong>Do</strong> until convergence:
<ul>
<li><em>Fit</em> <span class="math inline">\(h(W)\)</span> offset by <span class="math inline">\(Y^{**}\)</span></li>
<li><em>Predict</em> <span class="math inline">\(Y\)</span> using <span class="math inline">\(h(W)\)</span> with no offset to get new <span class="math inline">\(Y^*\)</span></li>
<li><em>Predict</em> <span class="math inline">\(Y\)</span> using <span class="math inline">\(h(W)\)</span> with offset (<span class="math inline">\(h(A,W)\)</span>)</li>
<li><em>Fit</em> <span class="math inline">\(f(A)\)</span> offset by <span class="math inline">\(Y^{*}\)</span></li>
<li><em>Predict</em> <span class="math inline">\(Y\)</span> using <span class="math inline">\(f(A)\)</span> with no offset to get new <span class="math inline">\(Y^{**}\)</span></li>
<li><em>Predict</em> <span class="math inline">\(Y\)</span> using <span class="math inline">\(f(A)\)</span> with offset (<span class="math inline">\(f(A,W)\)</span>)</li>
</ul></li>
</ul>
<p><em>Where</em> convergence is defined as no change in the difference between <span class="math inline">\(h(A,W)\)</span> and <span class="math inline">\(f(A,W)\)</span> between iterations based on a threshold (by default this value is defined as <span class="math inline">\(\delta\)</span> = 0.001). In this way, we can say that the two models have converged to the same model.</p>
<p>This algorithm is applied to <span class="math inline">\(A\)</span> as a mixture to generate a decision tree that includes multiple different exposures in <span class="math inline">\(A\)</span> while adjusting for <span class="math inline">\(W\)</span> non-parametrically Likewise, the backfitting algorithm is applied to each individual component of <span class="math inline">\(A\)</span>, <span class="math inline">\(A_i\)</span>, while controlling for other mixture variables <span class="math inline">\(A_{\ne i}\)</span> and <span class="math inline">\(W\)</span>. In this way, univariate thresholds are determined for each mixture variable individually. This backfitting procedure allows us to estimate <span class="math inline">\(Y = f(A) + h(W)\)</span> such that we are able to fit our decision tree models <span class="math inline">\(f(A)\)</span> while adjusting for the covariate model <span class="math inline">\(h(W)\)</span>. Convergence is defined as an absolute difference between <span class="math inline">\(f(A,W)\)</span> and <span class="math inline">\(h(A,W)\)</span></p>
</div>
<div id="mixture-rule-ensemble-fitting" class="section level2">
<h2>Mixture Rule Ensemble Fitting</h2>
<p>The <a href="https://github.com/marjoleinF/pre"><code>pre</code> package</a><span class="citation">(Fokkema 2020)</span> is used to fit rule ensembles on the mixture components modeled together. Within the respective parameter generating sample, <code>At</code>, the <code>pre</code> package is fit to the min-max scaled outcome <span class="math inline">\(Y\)</span>. The parameters passed to <code>pre</code> are: <code>use.grad = FALSE</code>, <code>tree.unbiased = TRUE</code>, <code>removecomplements = TRUE</code>, <code>removeduplicates = TRUE</code>, <code>maxdepth = pre::maxdepth_sampler()</code>, <code>sampfrac = min(1, (11 * sqrt(dim(At)[1]) + 1) / dim(At)[1])</code>, <code>nfolds = 10</code>; meaning that the tree is fit to the mixture without gradient boosting (using glmtree instead of ctree), the unbiased tree generation algorithm is used from <code>pre</code>, we remove identical trees and trees found in earlier rule ensembles, the max depth sampler randomly generates different depths of trees for the ensemble partitioning, the sampling fraction of observations used to create the rules is set to it’s default value in the <code>pre</code> package and we use 10 fold cross-validation. The <code>offset</code> parameter takes the <span class="math inline">\(Y^{*}\)</span> predictions during backfitting. In this way, because of the use of cross-validation (CV) to identify the size of the linear combination of decision trees, this approaches invokes in it the Super Learner methodology such that we are guaranteed asymptotically to choose the correct set of nodes that explains the underlying joint distribution.</p>
</div>
<div id="marginal-rule-decision-tree-fitting" class="section level2">
<h2>Marginal Rule Decision Tree Fitting</h2>
<p>We create a new learner for the <a href="https://github.com/tlverse/sl3"><code>sl3</code>package</a> <span class="citation">(Coyle et al. 2021)</span> to allow for ensemble machine learning of individual decision tree algorithms. This new learner is called <code>Lrnr_glmtree</code> and uses the <a href="http://partykit.r-forge.r-project.org/partykit/"><code>partykit</code> package</a>[partykit2015], this <code>Lrnr_glmtree</code> function takes in various hyper-parameters for <code>glmtree</code> and constructs a Super Learner based on <code>alpha</code>, <code>prune type</code>, <code>minsize</code> and <code>max-depth</code>. The <code>alpha</code> parameter specifies the p-value for any parameter stability test in order to make a node split - a node is split if p-value falls below <code>alpha</code>. <code>prune = AIC</code> indicates that the <code>AIC</code> post-pruning is performed on the tree. <code>minsize</code> is the minimum number of observations in a node. <code>maxdepth</code> is the maximum depth of the tree. As we will see later, we create a <code>Super Learner</code> of decision trees by varying these hyper-parameters in order to find the decision tree that best fits our data.</p>
</div>
<div id="estimates-for-consistent-trees" class="section level2">
<h2>Estimates for Consistent Trees</h2>
<p><code>CVtreeMLE</code> ensures that estimates for trees found are stable by only including rules that have the same mixture variables included in them across all folds. For example, consider the analyst runs <code>CVtreeMLE</code> using 10-fold cross-validation and in all ten folds a rule was determined that had variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> - in this case, estimates of the ATE given exposure to this rule would be estimated. That is, in the recursive partitioning, a rule may be slightly different in where the variables are split, but if the same variables are found across all the folds, a target parameter is calculated for this rule. This can look like:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.1\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.0\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.3\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.3\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.5\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.3\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.1\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.5\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.1\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.5\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.3\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
</ol>
<p>Here, where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are partitioned is slightly different but the same two variables were included in a rule found in all the folds and therefore this is a consistent decision tree for these two variables. If however, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are not found in every fold of the data, this is considered an unstable tree, or interaction, and estimates are not determined. In this way, it is important to run <code>CVtreeMLE</code> with a high number of folds to ensure that the parameter-generating sample is large enough to accurately determined if there are interactions in the mixture space.</p>
</div>
<div id="rule-coverage" class="section level2">
<h2>Rule Coverage</h2>
<p>As seen above, <code>CVtreeMLE</code> will deliver estimates only for trees that have the same variables in them across all the folds; however, the actual tree may be slightly different across the folds. To check how stable a tree is, and therefore, how interpretable the estimates are as it relates to exact exposure levels determined, we use a Jaccard coefficient coverage measure defined as;</p>
<p><span class="math display">\[J(A,B) = \frac{|A \cap B|}{|A \cup B|}\]</span></p>
<p>In our case, the numerator is the number of observations that are included in all the rules (intersection) determined across the folds. The denominator is the total number of observations indicated by any of the rules found across the folds (union). The ratio then gives us a measure of how consistent the determined rules are in the trees. A coverage score of 100% indicates that the rules across the folds were exactly the same or rather the number of observations included in rules across all the folds were the same as the total number of individuals indicated by the rules in all folds. A coverage score of 80% indicates that 20% of individuals were not included in the respective rule for all the folds.</p>
</div>
<div id="creating-a-total-rule-across-folds" class="section level2">
<h2>Creating a Total Rule Across Folds</h2>
<p>For decision trees that include the same variables across the folds for the mixture variables, we create a total rule based on observations included in rules in any of the folds. Considering the example above:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.1\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.0\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.3\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.3\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.5\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.3\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.1\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.5\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.1\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.5\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.3\)</span></li>
<li><span class="math inline">\(X_1 &gt; 1.2\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.4\)</span></li>
</ol>
<p>The average rule would be <span class="math inline">\(X_1 &gt; 1.0\)</span> &amp; <span class="math inline">\(X_2 &lt; 0.5\)</span>. This is because this rule includes observations across all rules found across any fold. This rule would then be accompanied by a coverage score to indicate how many observations are indicated by this rule compared to the sum of all rules found across the folds. Of course, this pooled rule may be anti-conservative if there is high variability in the decision tree nodes. If there is strong stability, the pooled rule will be equal to the rules found across the folds.</p>
</div>
<div id="data-adaptively-determining-exposure-levels" class="section level2">
<h2>Data Adaptively Determining Exposure Levels</h2>
<p>As discussed, our exposure of interest <span class="math inline">\(A\)</span> is not defined <em>a priori</em>. We need to “discover” our exposure by applying a decision tree algorithm to our empirical data <span class="math inline">\(P_n\)</span>. Our algorithm is a recursive partitioning algorithm where the model’s objective function is used for estimating the parameters and the split points; the corresponding model scores are tested for parameter instability in each node to assess which variable should be used for partitioning. The benefits of employing this approach are: The objective function used for parameter estimation is also used for partitioning (testing and split point estimation). The recursive partitioning allows for modeling of nonlinear relationships and automated detection of interactions among the explanatory mixture variables.</p>
<p>Consider a parametric <span class="math inline">\(\mathcal{M}\)</span>(Y, <span class="math inline">\(\theta\)</span>) with (possibly vector-valued) observations $Y  $ and a k-dimensional vector of parameters <span class="math inline">\(\theta \in \Theta\)</span>. Given <span class="math inline">\(n\)</span> observations <span class="math inline">\(Y_i (i = 1, . . . , n)\)</span> the model can be fitted by minimizing some objective function <span class="math inline">\(\Psi(Y, \theta)\)</span> yielding the parameter estimate <span class="math inline">\(\hat{\theta}\)</span>,</p>
<p><span class="math display">\[\hat{\theta} = argmin_{\theta \in \Theta} \sum_{i = 1}^n\Psi(Y_i, \theta)\]</span></p>
<p>Estimators of this type include various well-known estimation techniques, the most popular being ordinary least squares (OLS) or maximum likelihood (ML) among other M-type estimators.</p>
<p>However, in many situations, it is unreasonable to assume that a single global model <span class="math inline">\(\mathcal{M}(Y, \theta)\)</span> fits all <span class="math inline">\(n\)</span> observations well. But it might be possible to partition the observations with respect to some variables such that a well-fitting model can be found locally in each cell of the partition. In such a situation, we can use a recursive partitioning approach based on <span class="math inline">\(\ell\)</span> partitioning variables <span class="math inline">\(Z_j \in Z_j ( j = 1, . . . , \ell)\)</span> to adaptively find a good approximation of this partition. More formally, we assume that a partition <span class="math inline">\(\{ \mathcal{B}_b\}b=1,...,B\)</span> of the space <span class="math inline">\(Z = Z_1 × ... × Z\ell\)</span> exists with <span class="math inline">\(B\)</span> cells (or segments) such that in each cell <span class="math inline">\(\mathcal{B}_b\)</span> a model <span class="math inline">\(\mathcal{M}(Y, \theta_b)\)</span> with a cell-specific parameter <span class="math inline">\(\theta_b\)</span> holds. We denote this segmented model by <span class="math inline">\(\mathcal{M}_B(Y, \{\theta_b\})\)</span> where <span class="math inline">\(\{\theta_b\}b=1,...,B\)</span> is the full combined parameter. Special cases of such segmented models are classification and regression trees where many partitioning variables <span class="math inline">\(Z_j\)</span> but only very simple models <span class="math inline">\(M\)</span> are used. For example, with regression trees a simple model <span class="math inline">\(\mathcal{M}\)</span> is chosen: the parameter <span class="math inline">\(\theta\)</span> describes the mean of the univariate observations <span class="math inline">\(Y_i\)</span> and is estimated by OLS (or equivalently ML). In more traditional CART based models, our <span class="math inline">\(\mathcal{M}\)</span> are usually the Gini impurity measure, entropy, or information gain.</p>
<p>Given the correct partition <span class="math inline">\(\{B_b\}\)</span> the estimation of the parameters <span class="math inline">\(\{\theta_b\}\)</span> that minimize the global objective function can easily be achieved by computing the locally optimal parameter estimates <span class="math inline">\(\hat{\theta}_b\)</span> in each segment <span class="math inline">\(\mathcal{B}_b\)</span>. However, if <span class="math inline">\(\{\mathcal{B}_b\}\)</span> is unknown, minimization of the global objective function:</p>
<p><span class="math display">\[argmin\sum_{b=1}^B \sum_{i \in I_b} \Psi (Y_i,\theta_b)\]</span></p>
<p>Therefore, we define this recursive partitioning model as <span class="math inline">\(h(A)\)</span>:</p>
<p><span class="math display">\[ h(A) = argmin\sum_{b=1}^B \sum_{i \in I_b} \Psi (Y_i,\theta_b)\]</span> And <span class="math inline">\(g(W)\)</span> as <span class="math inline">\(Q_n(W)\)</span> or a non-parametric estimator <span class="math inline">\(Y\)</span> given covariates <span class="math inline">\(W\)</span>. Our full additive model is then:</p>
<p><span class="math display">\[\hat{Y} = argmin\sum_{b=1}^B \sum_{i \in I_b} \Psi (Y_i,\theta_b) + Q_n(W) \]</span> Where <span class="math inline">\(Q_n\)</span> is a non-parameter estimator (Super Learner). Now, consider the partition node results <span class="math inline">\(B\)</span> found in <span class="math inline">\(h(A)\)</span> that minimize some objective loss function for <span class="math inline">\(\Psi\)</span>, we can denote exposure to this combination of levels of exposures levels <span class="math inline">\(b_i\)</span> found in the mixture <span class="math inline">\(A\)</span> as <span class="math inline">\(a_b(P_n) = 1\)</span>. That is, <span class="math inline">\(a_b(P_n) = 1\)</span> are observations that have exposure levels found in the nodes for <span class="math inline">\(\Psi\)</span>.</p>
<p>We then use these exposure groups identified through <span class="math inline">\(\Psi\)</span> as our <em>data-adaptive target parameter</em>:</p>
<p><span class="math display">\[\Psi_{a_b(P_n) = 1, a_b(P_n) = 0} (P) = E_P[E_P(Y | A = a_b(P_n) = 1, W) - E_P(Y | A = a_b(P_n) = 0, W)]\]</span> In words, this is the mean difference in the counterfactual outcomes if all individuals were exposed to <span class="math inline">\(a_b(P_n)\)</span> compared to if all individuals were unexposed. Again, <span class="math inline">\(a_b(P_n)\)</span> is a rule consisting of a set of thresholds which defines exposure and is found data-adaptively using a pre-specified model applied to the parameter-generating sample of the full data.</p>
<p>For which the substitution estimator is:</p>
<p><span class="math display">\[ \frac{1}{n} \sum_{i = 1} \{Q_n(a_b(P_n) = 1, W_i) -  Q_n(a_b(P_n) = 0, W_i)\} \]</span> Here, <span class="math inline">\(Q_n\)</span> is a non-parametric esitmator.</p>
<p>If we were to use the full data to both identify partitions <span class="math inline">\(b\)</span> in the mixture space and fit out substitution estimator, this dual use of the data will result in over-fitting bias; consider if our data-generating distribution where:</p>
<p><span class="math display">\[E_W\{E(Y | A = a_b(Pn) = 1, W) - E(Y | A = a_b(Pn) = 0, W)\} = 0 \]</span></p>
<p>Our substitution estimator would be positively biased. If we were to do sample splitting such that a training sample is used to defined to define <span class="math inline">\(a_b(Pn)\)</span> and a separate estimation sample is used to estimate <span class="math inline">\(E_W\{E(Y| A= a_b(P_n) = 1, W) - E(Y| A= a_b(P_n) = 0, W) \}\)</span>. In this way, deriving valid statistical inference is possible. However, by using such sample splitting methods, our power is reduced given the smaller number of observations used in the estimation sample.</p>
</div>
<div id="data-adaptive-target-parameter" class="section level2">
<h2>Data Adaptive Target Parameter</h2>
<p>V-fold cross-validation involves: (i) <span class="math inline">\({1,..., n}\)</span>, observations, is divided into <span class="math inline">\(V\)</span> equal size subgroups, (ii) for each <span class="math inline">\(v\)</span>, an estimation-sample, notationally <span class="math inline">\(P_{n,v}\)</span> , is defined by the v-th subgroup of size <span class="math inline">\(n/V\)</span>, while the parameter-generating sample, <span class="math inline">\(P_{n,v^c}\)</span>, is its complement. More concretely, for split <span class="math inline">\(v\)</span> the empirical distribution, <span class="math inline">\(P_{n,v^c}\)</span>, is used to define the exposure <span class="math inline">\(A\)</span> given some <span class="math inline">\(\textit{a priori}\)</span> algorithm which creates a summary measure of the mixture, the observations not in <span class="math inline">\(P_{n,v^c}\)</span>, namely the empirical distribution for <span class="math inline">\(P_{n,v}\)</span> then is used to generate the parameter of interest.</p>
<p>The choice of target parameter mapping and corresponding estimator mapping is informed by <span class="math inline">\(P_{n,v^c}\)</span> but not <span class="math inline">\(P_{n,v}\)</span>, the sample spit data-adaptive statistical target parameter <span class="math inline">\(\Psi_n : \mathcal{M} \rightarrow R\)</span> is:</p>
<p><span class="math display">\[ \Psi_n(P) = Ave\{ \Psi_{P_{n,v^c}}(P)\} \equiv \frac{1}{V} \sum_{v=1}^V \Psi_{P_{n,v^c}}(P)\]</span></p>
<p>As can be seen, this target parameter mapping depends on the data, the corresponding estimator can be written as:</p>
<p><span class="math display">\[ \psi_n = \hat{\Psi}(P_n) = Ave\{ \hat{\Psi}_{P_{n,v^c}}(P_{n,v})\} = \frac{1}{V} \sum_{v=1}^V \Psi_{P_{n,v^c}}(P_{n,v})\]</span> In words, this is the average treatment effect estimated in our estimation sample <span class="math inline">\(P_{n,v}\)</span>, based on decision rules and estimators derived in the parameter generating sample <span class="math inline">\(P_{n,v^c}\)</span>.</p>
<p>Our substitution estimator is thus:</p>
<p><span class="math display">\[\hat{\Psi}_{P_{n,v^c}}(P_{n,v}) = \frac{1}{n_v}\sum_{i:Z_i=v} \{Q_{n,v^c}(a_b(P_{n,v^c}) = 1, W_i) - Q_{n,v^c}(a_b(P_{n,v^c}) = 0, W_i) \}\]</span></p>
<p>where <span class="math inline">\(Q_{n,v^c} = \hat{Q}(P_{n,v^c})\)</span>. This is the difference in averages of predicted values (based on a fit of <span class="math inline">\(Q\)</span> in the parameter generating sample or <span class="math inline">\(Q_{n,v^c}\)</span>) across the observation in the estimation sample across covariates <span class="math inline">\(W_i\)</span> and the binary variable <span class="math inline">\(A\)</span> determined in the training sample <span class="math inline">\(a_b(P_{n,v^c})\)</span>.</p>
<p>Overall, in the CV procedure, in each fold we use the parameter-generating sample to identify some set of nodes <span class="math inline">\(b\)</span> in our <span class="math inline">\(h(A)\)</span> function. This decision tree is applied to the training data and the estimator <span class="math inline">\(Q_{n,v^c}\)</span> is created given the rule found in this fold and covariates for observations in this training fold. <span class="math inline">\(Q_{n,v^c}\)</span> is thus the outcome model, or <span class="math inline">\(Y|A,W\)</span>. The decision tree rule is then applied to the estimation sample data to create <span class="math inline">\(A\)</span> for the estimation sample. Now given an identified exposure for the estimation sample, we get the difference of averages of the predicted values by predicting the estimation sample data through our <span class="math inline">\(Q_{n,v^c}\)</span> which was trained on the parameter-generating sample.</p>
</div>
<div id="tmle-for-data-adaptive-parameters" class="section level2">
<h2>TMLE for Data-Adaptive Parameters</h2>
<p>In <code>CVtreeMLE</code>, the <span class="math inline">\(Q\)</span> is estimated as a very large semi-parametric model by using Super Learner. In doing so, our bias is reduced relative to estimation according to a misspecified parametric model. However, this substitution estimator is still overly biased and not asymptotically linear which makes robust statistical inference based on this estimator problematic. That is, our bias-variance trade-off is not optimized for our parameter of interest, which is the average treatment effect given our counterfactual of interest, at this initial stage, our estimator’s bias/variance trade-off is optimized for the entire density of <span class="math inline">\(Y\)</span>.</p>
<p>However, a targeted maximum likelihood estimator based on this initial estimator reduces bias and under weak assumptions has an asymptotically normal sampling distribution.</p>
<p>The efficient influence curve for <span class="math inline">\(\Psi_{a_b(P_n) = 1, a_b(P_n) = 0} (P) = E_P[E_P(Y | A = a_b(P_n) = 1, W) - E_P(Y | A = a_b(P_n) = 0, W)]\)</span> is given by:</p>
<p><span class="math display">\[D^*_{P_{n,v^c}}(O) = \{ \frac{I(A = a_b(P_{n,v^c}) = 1)}{g_0(a_b(P_{n,v^c}) = 1 | W)} - \frac{I(A = a_b(P_{n,v^c}) = 0)}{g_0(a_b(P_{n,v^c}) = 0 | W)}\}(Y - Q_0(A,W)) + Q_0(a_b(P_{n,v^c}) = 1,W) - Q_0(a_b(P_{n,v^c}) = 0,W) - \Psi_{a_b(P_n) = 1, a_b(P_n) = 0}(P_0) \]</span></p>
<p>This suggests the following least favorable submodel:</p>
<p><span class="math display">\[Logit Q_{n,v,\epsilon}(A,W) = Logit Q_{n,v}(A,W) + \epsilon H_{P_n,v^c}(A,W;g)\]</span></p>
<p>Where:</p>
<p><span class="math display">\[H_{P_{n,v^c}}(A,W;g) = \frac{I(A = a_b(P_{n,v^c}) = 1)}{g_0(a_b(P_{n,v^c}) = 1 | W)} - \frac{I(A = a_b(P_{n,v^c}) = 0)}{g_0(a_b(P_{n,v^c}) = 0 | W)}\]</span></p>
<p>and <span class="math inline">\(g(a|W) \equiv P(A=a|W)\)</span>. By estimating <span class="math inline">\(g\)</span> on the parameter-generating sample, we obtain this clever covariate, <span class="math inline">\(H_{P_{n,v^c}}(A,W;g_{n,v^c})\)</span>. That is, in the same fashion as our <span class="math inline">\(Q\)</span> estimator, we train a <span class="math inline">\(g\)</span> estimator to estimate the probability of exposure given covariates using the parameter-generating sample <span class="math inline">\(P_{n,v^c}\)</span> - then using this estimator we predict the probabilities of being exposed using data in the estimation sample <span class="math inline">\(P_{n,v}\)</span>. The resulting TMLE estimator for <span class="math inline">\(\Psi_{P_{n,v^c}}(P_0)\)</span> is:</p>
<p><span class="math display">\[\hat{\Psi}^{TMLE}_{P_n,v^c}(P_{n,v}) = \frac{1}{n_v}\sum_{i:Z_i=v} \{Q_{n,v^c, \epsilon}(a_b(P_{n,v^c}) = 1, W_i) - Q_{n,v^c, \epsilon}(a_b(P_{n,v^c}) = 0, W_i) \} \]</span></p>
<p>That is, in each of our <span class="math inline">\(v\)</span> folds, indicating our estimation sample, calculate the TMLE updated expected difference in counterfactual means if all individuals in the estimation sample were exposed to the mixture rule compared to if no individuals were exposed. Here, the mixture rule is determined in the parameter-generating sample, as are the initial estimators for <span class="math inline">\(Q\)</span> and <span class="math inline">\(g\)</span> - using these estimators, a clever covariate and least favorable submodel is created to fluctuate the initial predictions in the estimation sample to create asymptotically unbiased estimates of our target parameter.</p>
<p>The estimated influence curve at each observation <span class="math inline">\(O_i\)</span> in the estimation sample <span class="math inline">\(P_{n,v}\)</span> is given by:</p>
<p><span class="math display">\[D^*_{n,v,P_{n,v^c}}(O) = \{ \frac{I(A = a_b(P_{n,v^c}) = 1)}{g_{n,v^c}(a_b(P_{n,v^c}) = 1 | W)} - \frac{I(A = a_b(P_{n,v^c}) = 0)}{g_{n,v^c}(a_b(P_{n,v^c}) = 0 | W)}\}(Y - Q_{n,v^c,\epsilon_{n,v}}(A,W)) + [ Q_{n,v^c,\epsilon_{n,v}}(a_b(P_{n,v^c}) = 1,W) - Q_0(a_b(P_{n,v^c}) = 0,W) ] - \hat{\Psi}^{TMLE}_{P_{n,v^c}}(P_{n,v}) \]</span></p>
<p>The estimated influence curve values estimated in the above equation provide us with an estimate of the standard error of the TMLE parameter:</p>
<p><span class="math display">\[se(\hat{\Psi}_{P_n,v^c}^{TMLE}) = \sqrt{\frac{\hat{var}(D^*_{n,v,P_{n,v^c}}(O))}{n/V}}\]</span></p>
<p>We combine our v-specific TMLEs across the estimation samples by taking a simple average of our parameter across the folds:</p>
<p><span class="math display">\[\hat{\Psi}(P_n) = \frac{1}{V} \sum_{v=1}^V \hat{\Psi}^{TMLE}_{P_n,v^c}(P_{n,v})\]</span></p>
<p>Similarly, the asymptotic variance of this estimator can also simply be averaged across the folds:</p>
<p><span class="math display">\[\sigma^2_n = \frac{1}{V}\sum_{v=1}^V P_{n,v}(D^*_{n,v,P_{n,v^c}})^2\]</span> And likewise, we calculate the standard error for our target parameter in the usual way:</p>
<p><span class="math display">\[se(\hat{\Psi}(P_n)) = \sigma_n / \sqrt{n}\]</span> With corresponding 0.95-confidence intervals as <span class="math inline">\(\psi_n \pm 1.96 \sigma_n / \sqrt{n}\)</span></p>
</div>
<div id="conditions-for-asymptotic-linearity-for-tmle" class="section level2">
<h2>Conditions for asymptotic linearity for TMLE</h2>
<p>Let <span class="math inline">\(\Psi(P_0)\)</span> be the parameter of interest and <span class="math inline">\(\Psi(P^*_n)\)</span> be the TMLE estimator. We can write the linearization of the TMLE estimator as:</p>
<p><span class="math display">\[\Psi(P^*_n) - \Psi(P_0) = (P^*_n - P_0)D(P^*_n) + R(n)\]</span> In words, the estimator minus truth can be written as an empirical process multiplied by the efficient influence curve (EIC) plus some second order remainder. The difference between the estimator and true value can be treated as an i.i.d sum of influence curves, the i.i.d sum will be normally distributed with variance equal to variance of the IC over sample size.</p>
<p>Applying empirical process theory shows that asymptotic linearity requires that <span class="math inline">\(D(P^*_n)\)</span> falls in a <span class="math inline">\(P_0\)</span>-Donsker class with probability tending to 1. Without going into details, this requires that the estimators <span class="math inline">\(Q_n\)</span> and <span class="math inline">\(g_n\)</span> for <span class="math inline">\(Q_0(A,W) \equiv E(Y | A = a, W)\)</span> and <span class="math inline">\(g_0(W) \equiv P_0(A = 1, W)\)</span> are not too adaptive. If these estimators are too adaptive you get dependence in the IC within a study - CLT breaks down, and therefore we would need to be careful about what learners are in SL to ensure the learners are not too data-adaptive.</p>
<p>In the standard TMLE case then, TMLE suffers when the initial estimator is too adaptive - this leaves very little signal in the data to fit the residual bias with respect to the initial estimator in the targeting step. In order to rely on the central limit theorem for statistical inference, empirical process conditions put bounds on how adaptive the initial estimator can be. However, we would like to have a method that does not have these limitation - so we can have a true statistical machine - that is we don’t have to worry about what goes in the Super Learner library.</p>
</div>
<div id="cv-tmle" class="section level2">
<h2>CV-TMLE</h2>
<p><em>CV-TMLE</em> allows for more flexible estimators without worry of over-fitting. It has been formally established (citation) that CV-TMLE asymptotics, under stated conditions, avoid previous empirical process conditions necessary for TMLE. Implications of this theorem better allow super learning for construction of semiparametric efficient estimators of target parameters. As stated above <code>CVtreeMLE</code> implements <em>CV-TMLE</em> by partitioning the data such that estimators for <span class="math inline">\(Q\)</span> and <span class="math inline">\(g\)</span> are trained in the parameter-generating portion of the data and the TMLE update of the target parameter is done on the estimation samples. In this way, our estimators are more independent: just like an average of i.i.d random variables, which doesn’t require entropy conditions any more. Thus suggests that one can establish a CLT for this cross-validated empirical process term without having to enforce restricting entropy conditions (that thereby limit the adaptiveness of the initial estimators.). More simply, by partitioning the data we can avoid over-fitting and ensure there is enough bias in our initial estimates such that targeting towards our parameter of interest is possible. As such, when using <code>CVtreeMLE</code> the analyst does not need to be concerned about highly adaptive estimators being included in the Super Learner library.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In sum, <code>CVtreeMLE</code> is a statistical approach, built on asympototic theory for semi-parametric estimators, that allows for robust target parameters to be derived from decision tree results. The goal is to provide interpretable and reliable results for analysts who are interested in statistical estimation of mixed exposures. Below we now show examples of how to use <code>CVtreeMLE</code> with more detailed interpretations of results.</p>
<p>To start, let’s load the packages we’ll need and set a seed for simulation:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(CVtreeMLE)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sl3)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(qgcomp)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Attaching package: &#39;dplyr&#39;</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; The following object is masked from &#39;package:kableExtra&#39;:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     group_rows</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; The following objects are masked from &#39;package:data.table&#39;:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     between, first, last</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; The following objects are masked from &#39;package:stats&#39;:</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     filter, lag</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; The following objects are masked from &#39;package:base&#39;:</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     intersect, setdiff, setequal, union</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11249</span>)</span></code></pre></div>
</div>
<div id="simulate-data" class="section level2">
<h2>Simulate Data</h2>
<p>Let’s start by simulating data in a similar way as we did in the <code>README.md</code> tutorial:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a> <span class="co"># number of observations we want to simulate</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n_obs <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split points for each mixture</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>splits <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">2.5</span>, <span class="fl">3.6</span>) </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># minimum values for each mixture</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>mins <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># maximum value for each mixture</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>maxs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>) </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># mu for each mixture</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># variance/covariance of mixture variables</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.7</span>, <span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">ncol =</span> <span class="dv">3</span>) </span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># subspace probability relationship with covariate W1</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>w1_betas <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.0</span>, <span class="fl">0.01</span>, <span class="fl">0.03</span>, <span class="fl">0.06</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.2</span>, <span class="fl">0.04</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># subspace probability relationship with covariate W2</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>w2_betas <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.0</span>, <span class="fl">0.04</span>, <span class="fl">0.01</span>, <span class="fl">0.07</span>, <span class="fl">0.15</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.04</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># probability of mixture subspace (for multinomial outcome generation)</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>mix_subspace_betas <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.00</span>, <span class="fl">0.08</span>, <span class="fl">0.05</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.033</span>, <span class="fl">0.07</span>, <span class="fl">0.09</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># mixture subspace impact on outcome Y, here the subspace where M1 is lower and M2 and M3 are higher based on values in splits</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>subspace_assoc_strength_betas <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># marginal impact of mixture component on Y</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>marginal_impact_betas <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>) </span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># random error</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>eps_sd <span class="ot">&lt;-</span> <span class="fl">0.01</span> </span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># if outcome is binary</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>binary <span class="ot">&lt;-</span> <span class="cn">FALSE</span></span></code></pre></div>
<p>As discussed, the <code>subspace_assoc_strength_betas</code> parameter is used to indicate the subspace we want to use and the expected outcome in that subspace.</p>
<p>The indices correspond to an area in the cube:</p>
<ol style="list-style-type: decimal">
<li>All mixtures lower than specified thresholds</li>
<li>M1 is higher but M2 and M3 are lower</li>
<li>M2 is higher but M1 and M3 are lower</li>
<li>M1 and M2 are higher and M3 is lower</li>
<li>M3 is higher and M1 and M2 are lower</li>
<li>M1 and M3 are higher and M2 is lower</li>
<li>M2 and M3 are higher and M1 is lower</li>
<li>All mixtures are higher than thresholds</li>
</ol>
<p>Therefore, we are simulating here an outcome of 1.2 when this rule is met <span class="math inline">\(M_1 &gt; 0.8\)</span> &amp; <span class="math inline">\(M_2 &lt; 2.5\)</span> &amp; <span class="math inline">\(M_3 &lt; 3.6\)</span>. The outcome is 0 in all other spaces around the mixture cube.</p>
<p>Let’s create this data:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>sim_data <span class="ot">&lt;-</span> <span class="fu">simulate_mixture_cube</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_obs =</span> n_obs, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">splits =</span> splits,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">mins =</span> mins,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">maxs =</span> maxs,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu =</span> mu,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">sigma =</span> sigma,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">w1_betas =</span> w1_betas,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">w2_betas =</span> w2_betas,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">mix_subspace_betas =</span> mix_subspace_betas,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">subspace_assoc_strength_betas =</span> subspace_assoc_strength_betas,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">marginal_impact_betas =</span> marginal_impact_betas,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">eps_sd =</span> eps_sd,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">binary =</span> binary</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(sim_data) <span class="sc">%&gt;%</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kbl</span>(<span class="at">caption =</span> <span class="st">&quot;Simulated Data&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_classic</span>(<span class="at">full_width =</span> F, <span class="at">html_font =</span> <span class="st">&quot;Cambria&quot;</span>)</span></code></pre></div>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Simulated Data
</caption>
<thead>
<tr>
<th style="text-align:right;">
W
</th>
<th style="text-align:right;">
W2
</th>
<th style="text-align:right;">
M1
</th>
<th style="text-align:right;">
M2
</th>
<th style="text-align:right;">
M3
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.1134482
</td>
<td style="text-align:right;">
-0.6861745
</td>
<td style="text-align:right;">
1.7367509
</td>
<td style="text-align:right;">
3.2719319
</td>
<td style="text-align:right;">
2.5817526
</td>
<td style="text-align:right;">
-0.7898209
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.6923931
</td>
<td style="text-align:right;">
0.7052709
</td>
<td style="text-align:right;">
0.0876085
</td>
<td style="text-align:right;">
0.4302742
</td>
<td style="text-align:right;">
0.5566116
</td>
<td style="text-align:right;">
0.0118347
</td>
</tr>
<tr>
<td style="text-align:right;">
0.1312834
</td>
<td style="text-align:right;">
-0.7761951
</td>
<td style="text-align:right;">
0.5275796
</td>
<td style="text-align:right;">
1.7378738
</td>
<td style="text-align:right;">
4.3237569
</td>
<td style="text-align:right;">
-0.6354813
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.4554513
</td>
<td style="text-align:right;">
-0.1063585
</td>
<td style="text-align:right;">
0.2913796
</td>
<td style="text-align:right;">
0.8267453
</td>
<td style="text-align:right;">
1.1960156
</td>
<td style="text-align:right;">
-0.5727259
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.4490637
</td>
<td style="text-align:right;">
0.9709884
</td>
<td style="text-align:right;">
0.0568035
</td>
<td style="text-align:right;">
3.0130119
</td>
<td style="text-align:right;">
0.7701573
</td>
<td style="text-align:right;">
0.5251671
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.5291928
</td>
<td style="text-align:right;">
0.5756340
</td>
<td style="text-align:right;">
2.6082538
</td>
<td style="text-align:right;">
3.7571418
</td>
<td style="text-align:right;">
4.7866524
</td>
<td style="text-align:right;">
0.0484475
</td>
</tr>
</tbody>
</table>
</div>
<div id="set-up-estimators-used-in-super-learners" class="section level2">
<h2>Set up Estimators used in Super Learners</h2>
<p>Here, we set up our Super Learner using <code>SL3</code> for the iterative backfitting procedure and the outcome mechanism <span class="math inline">\(Q\)</span>. These learners will fit <span class="math inline">\(Y|W\)</span> offset by <span class="math inline">\(Y|A\)</span> as we fit decision trees to the exposure variables both jointly and individially. This Super Learner will also fit <span class="math inline">\(Y|A,W\)</span> once rules are established for the <span class="math inline">\(Q\)</span> nuisance parameter needed for our target parameter of interest.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>lrnr_glm <span class="ot">&lt;-</span> Lrnr_glm<span class="sc">$</span><span class="fu">new</span>()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>lrnr_bayesglm <span class="ot">&lt;-</span> Lrnr_bayesglm<span class="sc">$</span><span class="fu">new</span>()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>lrnr_gam <span class="ot">&lt;-</span> Lrnr_gam<span class="sc">$</span><span class="fu">new</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># put all the learners together (this is just one way to do it)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">&lt;-</span> <span class="fu">c</span>(lrnr_glm, lrnr_bayesglm, lrnr_gam)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>Q1_stack <span class="ot">&lt;-</span> <span class="fu">make_learner</span>(Stack, learners)</span></code></pre></div>
</div>
<div id="build-the-decision-tree-super-learner" class="section level2">
<h2>Build the decision tree Super Learner</h2>
<p>Here we will now build our Super Learner made from decision trees from the <code>Lrnr_glmtree</code> learner in <code>sl3</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>lrnr_glmtree_001 <span class="ot">&lt;-</span> Lrnr_glmtree<span class="sc">$</span><span class="fu">new</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">maxdepth =</span> <span class="dv">3</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>lrnr_glmtree_002 <span class="ot">&lt;-</span> Lrnr_glmtree<span class="sc">$</span><span class="fu">new</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>,  <span class="at">maxdepth =</span> <span class="dv">4</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>lrnr_glmtree_003 <span class="ot">&lt;-</span> Lrnr_glmtree<span class="sc">$</span><span class="fu">new</span>(<span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">maxdepth =</span> <span class="dv">2</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>lrnr_glmtree_004 <span class="ot">&lt;-</span> Lrnr_glmtree<span class="sc">$</span><span class="fu">new</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>, <span class="at">maxdepth =</span> <span class="dv">1</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">&lt;-</span> <span class="fu">c</span>( lrnr_glmtree_001, lrnr_glmtree_002, lrnr_glmtree_003, lrnr_glmtree_004)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>discrete_sl_metalrn <span class="ot">&lt;-</span> Lrnr_cv_selector<span class="sc">$</span><span class="fu">new</span>()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>tree_stack <span class="ot">&lt;-</span> <span class="fu">make_learner</span>(Stack, learners)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>discrete_tree_sl <span class="ot">&lt;-</span> Lrnr_sl<span class="sc">$</span><span class="fu">new</span>(</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">learners =</span> tree_stack,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">metalearner =</span> discrete_sl_metalrn</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="run-cvtreemle" class="section level2">
<h2>Run <code>CVtreeMLE</code></h2>
<p>We will now pass the simulated data, learners, and variable names for each node in <span class="math inline">\(O = W,A,Y\)</span> to the <code>CVtreeMLE</code> function. <strong>NOTE</strong> in this vignette we are only using 2-fold CV for quick computation times in construction of the vignette. In reality, the user should use at least 10-fold CV.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>ptm <span class="ot">&lt;-</span> <span class="fu">proc.time</span>()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>sim_results <span class="ot">&lt;-</span> <span class="fu">CVtreeMLE</span>(<span class="at">data =</span> sim_data,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">W =</span> <span class="fu">c</span>(<span class="st">&quot;W&quot;</span>, <span class="st">&quot;W2&quot;</span>),</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Y =</span> <span class="st">&quot;y&quot;</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                         <span class="at">A =</span> <span class="fu">c</span>(<span class="fu">paste</span>(<span class="st">&quot;M&quot;</span>, <span class="fu">seq</span>(<span class="dv">3</span>), <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                         <span class="at">back_iter_SL =</span> Q1_stack,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                         <span class="at">tree_SL =</span> discrete_tree_sl, </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>                         <span class="at">n_folds =</span> <span class="dv">2</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>                         <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="fu">proc.time</span>() <span class="sc">-</span> ptm</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    user  system elapsed </span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 222.709   6.670 122.199</span></span></code></pre></div>
<p>Let’s first look at the RMSE for the iterative back-fitting models. Because our rules are determined in these models, from which our target parameter is derived it’s important that our models fit well. Given our simulated data, we would expect the mixture model to have the lowest RMSE.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>RMSE_results <span class="ot">&lt;-</span> sim_results<span class="sc">$</span><span class="st">`</span><span class="at">Model RMSEs</span><span class="st">`</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>RMSE_results <span class="sc">%&gt;%</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(<span class="at">caption =</span> <span class="st">&quot;Model Fit Results&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">kable_classic</span>(<span class="at">full_width =</span> F, <span class="at">html_font =</span> <span class="st">&quot;Cambria&quot;</span>)</span></code></pre></div>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Model Fit Results
</caption>
<thead>
<tr>
<th style="text-align:left;">
Var(s)
</th>
<th style="text-align:right;">
RMSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
M1
</td>
<td style="text-align:right;">
0.0992040
</td>
</tr>
<tr>
<td style="text-align:left;">
M2
</td>
<td style="text-align:right;">
0.1072759
</td>
</tr>
<tr>
<td style="text-align:left;">
M3
</td>
<td style="text-align:right;">
0.1086911
</td>
</tr>
<tr>
<td style="text-align:left;">
M1M2M3
</td>
<td style="text-align:right;">
0.0452928
</td>
</tr>
</tbody>
</table>
<p>The our mixture decision tree model has the lowest RMSE.</p>
<p>A popular method in mixtures currently is the quantile-sum g-computation method (keil citation). Let’s compare our model fit for this simulated data to that of quantile-sum g-computation.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a> qgcomp_additive_model <span class="ot">&lt;-</span> <span class="fu">qgcomp</span>(y<span class="sc">~</span>M1<span class="sc">+</span>M2<span class="sc">+</span>M3<span class="sc">+</span>W<span class="sc">+</span>W2, <span class="at">expnms=</span><span class="fu">c</span>(<span class="st">&quot;M1&quot;</span>, <span class="st">&quot;M2&quot;</span>, <span class="st">&quot;M3&quot;</span>), <span class="at">data=</span>sim_data)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">predict</span>(qgcomp_additive_model<span class="sc">$</span>fit) <span class="sc">-</span> sim_data<span class="sc">$</span>y)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.7298008</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>qgcomp_multi_model <span class="ot">&lt;-</span> <span class="fu">qgcomp</span>(y<span class="sc">~</span>M1<span class="sc">*</span>M2<span class="sc">*</span>M3<span class="sc">+</span>W<span class="sc">+</span>W2, <span class="at">expnms=</span><span class="fu">c</span>(<span class="st">&quot;M1&quot;</span>, <span class="st">&quot;M2&quot;</span>, <span class="st">&quot;M3&quot;</span>), <span class="at">data=</span>sim_data)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">predict</span>(qgcomp_multi_model<span class="sc">$</span>fit) <span class="sc">-</span> sim_data<span class="sc">$</span>y)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5220524</span></span></code></pre></div>
<p>As we can see, our results return a much smaller RMSE, which means our models fit this simulated data better.</p>
<p>We can look at the pooled TMLE results for this model:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>pooled_mixture_results <span class="ot">&lt;-</span> sim_results<span class="sc">$</span><span class="st">`</span><span class="at">Pooled TMLE Mixture Results</span><span class="st">`</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>pooled_mixture_results <span class="sc">%&gt;%</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(<span class="at">caption =</span> <span class="st">&quot;Pooled TMLE Mixture Results&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">kable_classic</span>(<span class="at">full_width =</span> F, <span class="at">html_font =</span> <span class="st">&quot;Cambria&quot;</span>)</span></code></pre></div>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Pooled TMLE Mixture Results
</caption>
<thead>
<tr>
<th style="text-align:right;">
Mixture ATE
</th>
<th style="text-align:right;">
Standard Error
</th>
<th style="text-align:right;">
Lower CI
</th>
<th style="text-align:right;">
Upper CI
</th>
<th style="text-align:right;">
P-value
</th>
<th style="text-align:right;">
P-value Adj
</th>
<th style="text-align:left;">
Vars
</th>
<th style="text-align:right;">
RMSE
</th>
<th style="text-align:left;">
Mixture Interaction Rules
</th>
<th style="text-align:right;">
Fraction Covered
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
2.838
</td>
<td style="text-align:right;">
0.099
</td>
<td style="text-align:right;">
2.644
</td>
<td style="text-align:right;">
3.032
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
M1M2M3
</td>
<td style="text-align:right;">
0.346
</td>
<td style="text-align:left;">
M1 &gt; 0.783 &amp; M1 &lt; 2.963 &amp; M2 &gt; 0.062 &amp; M2 &lt; 2.471 &amp; M3 &gt; 0.09 &amp; M3 &lt; 3.497
</td>
<td style="text-align:right;">
0.8275862
</td>
</tr>
</tbody>
</table>
<p>*Note - results in explanations below may change slightly based on runs:</p>
<p>Above, the mixture ATE for this rule is 2.92 (2.73 - 3.10), which covers our true ATE used to generate the data which was 3. The mixture ATE is interpreted as: the average counterfactual mean outcome if all individuals were exposed to the rule shown in <code>Mixture Interaction Rules</code> compared to if all individuals were unexposed is 2.92. That is, those individuals who are exposed to this rule have an outcome that is 2.92 higher compared to those that are not exposed to this rule. The standard error, confidence intervals and p-values are derived from the influence curve of this estimator as described above.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>mixture_v_results <span class="ot">&lt;-</span> sim_results<span class="sc">$</span><span class="st">`</span><span class="at">V-Specific Mix Results</span><span class="st">`</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>mixture_v_results<span class="sc">$</span>M1M2M3 <span class="sc">%&gt;%</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(<span class="at">caption =</span> <span class="st">&quot;V-Fold Mixture Results&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">kable_classic</span>(<span class="at">full_width =</span> F, <span class="at">html_font =</span> <span class="st">&quot;Cambria&quot;</span>)</span></code></pre></div>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
V-Fold Mixture Results
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Mixture ATE
</th>
<th style="text-align:left;">
Standard Error
</th>
<th style="text-align:left;">
Lower CI
</th>
<th style="text-align:left;">
Upper CI
</th>
<th style="text-align:left;">
P-value
</th>
<th style="text-align:left;">
P-value Adj
</th>
<th style="text-align:left;">
RMSE
</th>
<th style="text-align:left;">
Mixture Interaction Rules
</th>
<th style="text-align:left;">
Variables
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
2.591
</td>
<td style="text-align:left;">
0.206
</td>
<td style="text-align:left;">
2.187
</td>
<td style="text-align:left;">
2.995
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0.505
</td>
<td style="text-align:left;">
M2 &lt;= 2.421 &amp; M3 &lt;= 3.497 &amp; M1 &gt; 0.783
</td>
<td style="text-align:left;">
M1M2M3
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
2.858
</td>
<td style="text-align:left;">
0.021
</td>
<td style="text-align:left;">
2.818
</td>
<td style="text-align:left;">
2.899
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0.197
</td>
<td style="text-align:left;">
M1 &gt; 1.095 &amp; M2 &lt;= 2.471 &amp; M3 &lt;= 3.555
</td>
<td style="text-align:left;">
M1M2M3
</td>
</tr>
<tr>
<td style="text-align:left;">
Pooled
</td>
<td style="text-align:left;">
2.855
</td>
<td style="text-align:left;">
0.207
</td>
<td style="text-align:left;">
2.4494
</td>
<td style="text-align:left;">
3.2611
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0.2
</td>
<td style="text-align:left;">
M1 &gt; 0.783 &amp; M1 &lt; 2.963 &amp; M2 &gt; 0.062 &amp; M2 &lt; 2.471 &amp; M3 &gt; 0.09 &amp; M3 &lt; 3.497
</td>
<td style="text-align:left;">
M1M2M3
</td>
</tr>
</tbody>
</table>
<p>We can plot our v-fold mixture results findings using the <code>plot_mixture_results</code> function. This will return a list of plots with names corresponding to the interactions found.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>mixture_plots <span class="ot">&lt;-</span> <span class="fu">plot_mixture_results</span>(<span class="at">v_intxn_results =</span> sim_results<span class="sc">$</span><span class="st">`</span><span class="at">V-Specific Mix Results</span><span class="st">`</span>,<span class="at">hjust =</span> <span class="fl">0.8</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>mixture_plots<span class="sc">$</span>M1M2M3</span></code></pre></div>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAwAAAAEgCAYAAAADshSRAAAEDmlDQ1BrQ0dDb2xvclNwYWNlR2VuZXJpY1JHQgAAOI2NVV1oHFUUPpu5syskzoPUpqaSDv41lLRsUtGE2uj+ZbNt3CyTbLRBkMns3Z1pJjPj/KRpKT4UQRDBqOCT4P9bwSchaqvtiy2itFCiBIMo+ND6R6HSFwnruTOzu5O4a73L3PnmnO9+595z7t4LkLgsW5beJQIsGq4t5dPis8fmxMQ6dMF90A190C0rjpUqlSYBG+PCv9rt7yDG3tf2t/f/Z+uuUEcBiN2F2Kw4yiLiZQD+FcWyXYAEQfvICddi+AnEO2ycIOISw7UAVxieD/Cyz5mRMohfRSwoqoz+xNuIB+cj9loEB3Pw2448NaitKSLLRck2q5pOI9O9g/t/tkXda8Tbg0+PszB9FN8DuPaXKnKW4YcQn1Xk3HSIry5ps8UQ/2W5aQnxIwBdu7yFcgrxPsRjVXu8HOh0qao30cArp9SZZxDfg3h1wTzKxu5E/LUxX5wKdX5SnAzmDx4A4OIqLbB69yMesE1pKojLjVdoNsfyiPi45hZmAn3uLWdpOtfQOaVmikEs7ovj8hFWpz7EV6mel0L9Xy23FMYlPYZenAx0yDB1/PX6dledmQjikjkXCxqMJS9WtfFCyH9XtSekEF+2dH+P4tzITduTygGfv58a5VCTH5PtXD7EFZiNyUDBhHnsFTBgE0SQIA9pfFtgo6cKGuhooeilaKH41eDs38Ip+f4At1Rq/sjr6NEwQqb/I/DQqsLvaFUjvAx+eWirddAJZnAj1DFJL0mSg/gcIpPkMBkhoyCSJ8lTZIxk0TpKDjXHliJzZPO50dR5ASNSnzeLvIvod0HG/mdkmOC0z8VKnzcQ2M/Yz2vKldduXjp9bleLu0ZWn7vWc+l0JGcaai10yNrUnXLP/8Jf59ewX+c3Wgz+B34Df+vbVrc16zTMVgp9um9bxEfzPU5kPqUtVWxhs6OiWTVW+gIfywB9uXi7CGcGW/zk98k/kmvJ95IfJn/j3uQ+4c5zn3Kfcd+AyF3gLnJfcl9xH3OfR2rUee80a+6vo7EK5mmXUdyfQlrYLTwoZIU9wsPCZEtP6BWGhAlhL3p2N6sTjRdduwbHsG9kq32sgBepc+xurLPW4T9URpYGJ3ym4+8zA05u44QjST8ZIoVtu3qE7fWmdn5LPdqvgcZz8Ww8BWJ8X3w0PhQ/wnCDGd+LvlHs8dRy6bLLDuKMaZ20tZrqisPJ5ONiCq8yKhYM5cCgKOu66Lsc0aYOtZdo5QCwezI4wm9J/v0X23mlZXOfBjj8Jzv3WrY5D+CsA9D7aMs2gGfjve8ArD6mePZSeCfEYt8CONWDw8FXTxrPqx/r9Vt4biXeANh8vV7/+/16ffMD1N8AuKD/A/8leAvFY9bLAAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAMAoAMABAAAAAEAAAEgAAAAAIA3fPQAAEAASURBVHgB7J0FmBTH1obPCu6uIYRAnAgR4h7i7u7ucpP/3uQmxN3d3d3d3T3EQyC4E9zmr7dIze3pHV1ml9nlOzzLtFRXVb9V033OqVM1ZQknJhEBERABERABERABERABEVgkCJQvEnepmxQBERABERABERABERABEfAEZACoI4iACIiACIiACIiACIjAIkRABsAi1Ni6VREQAREQAREQAREQARGQAaA+IAIiIAIiIAIiIAIiIAKLEAEZAItQY+tWRUAEREAEREAEREAEREAGgPqACIiACIiACIiACIiACCxCBGQALEKNrVsVAREQAREQAREQAREQARkARewDP/30k2288cZ22WWXVTvXiRMn2qhRo6p9fW1eyP1G5dRTT7Utt9wyeqhWt59++mlbf/31rXXr1tavX79aLZvCrr32Wt/+9IFPPvkkY/nTp0+3zTbbzKe95ZZbMqYjj9VWW83+/PPPlDSTJ09OlnP00UennIvv3Hrrrcm0M2fOTDlN/ocffrg/v+2229oZZ5xh48aNS0lTrLI+/PBDO/DAA23DDTe0I4880h5//PGUcrQjAiIgAiIgAiJQiwT4ITBJcQg4hYofVUsccsgh1crwvffeS3To0CHx5ptvVuv62rxor732SmywwQYpRQ4YMCDRpEmTlGO1tfPXX3/5slu0aJHYfffdE2eeeWZtFZ0s55hjjvHtTx9winnyeHzjkUceSab717/+FT/t94cPH57o2rWrT/fjjz+mpBk7dmzy+oqKisTIkSNTzkd3lltuuWTaqVOnJk9dfvnlibKyskR5eXnCGSOJpZde2qdr165d4rvvvkumK0ZZzjDzedM3tt9+++R9HXvssYm5c+cmy9KGCIiACIiACIhA7RDQCEAtGlu5ivryyy9tzJgxuZKVxPmXX365Sj2uv/56++CDD6ocr40DP/zwg+FZP+GEE+yhhx6ys88+uzaKTVuGM+K8h3vevHlpz1O/Zs2apT3HwVdffdXWXnttc0ZAxjScoBynQNsTTzyRNt23335rcImX9fPPP9tpp51mzjiwIUOG2CuvvGLOyDBGUBgBOPjgg6vkV92yqN///d//2ZJLLmnOULGnnnrKl7nPPvv4EZPXX3+9Slk6IAIiIAIiIAIiULMEZADULN8quaMQ/fHHH15xq3IyjwPO022ECeUS0kybNi1tsmHDhnnl0tmYac+Hg7Nnz7bff//dZs2aFQ5l/ezdu7etvPLKadP8/fff/r4zKcXxi5znuUo4SjxNdH/SpEl+d9lll40eTm4XUn42dskMs2zsvPPOXtl99913q6QipOaFF16w7bbbrso5Dhx11FHmRlIMToQJZROMhM6dO5sbUUibDEOjffv2ttZaa6Wcf+mll2zOnDl20EEHWbdu3ZLnqNPqq6/uw5fgFZXqluVGs4z+duKJJ1rLli19lm7UwkLoEsaORAREQAREQAREoHYJyACoYd7EPjdv3tyef/55O/TQQ61Vq1bWq1cvc6EqXikKyvV+++1nJ598sq8NcfQodkEwGvBou/AMW2yxxaxNmza2zDLLWNx7usUWW3il8cYbb/TeYby2bCPMK3ChMV4J6969u1f8qAMK55QpU0JR/vPzzz+3zTff3MfS47ml/qRDaSTun308xSi4bF9zzTX+Ohfe4cuNZkYdMQpQ/sJ942EOCntISww68eEuDMpcOIrPB+UVL3W2eHqu79+/v+FRRogzp054tZF8y8/GDkWcPPGc5yMYACi5jz76aJXkeNlp8912263KOQ7QT4iR/+qrr3LOY6CMXXfd1d55552080YwAHbZZRdr0KBBSlk77LCDN0JglU5caFCVw9Uta9NNN/VtzT1FZcKECX63Z8+e0cPaFgEREAEREAERqAUCMgBqGDKeXBd77ZW6N954w0455RS77bbbDE/1VVdd5cMgqMJhhx1me+yxh68NhkBQqjmAcjtw4EDr27ev3Xvvvfbwww97QwLFFIUxyIwZM8zFbxuTcbfZZhtbddVVbaWVVjI8+WussYZX+giR4ZorrrjCK9cYCJdeemnIwn799VfbaqutbNCgQb5upMVwIB1/Xbp0sZtvvtkrxEsttZTfDp5qyudegxAOhDcbZf/22283woYwJO666y5bZ511UkYWQt2pN0r/3Xff7e8Dg2Prrbc2zmeSs846y09m5bybf+HrtMIKK/hwpELLj7MjT9oQIyxf6dSpkzdmmOgaH/FAKXdzJ8zF96fNDuPrhhtu8EZe2gSxgxgSlBGfVPvpp5/60Zs999wzdoVZjx49/GRtDMmoEDLEdfQZjMO4VKeskIeba+A3MURpfyYfM2KEsSQRAREQAREQARGoZQK1M9Vg0Sgl3SRgJva6Jk04pSsRnYQ5evRoPwHTKcJJOG4VGZ82OgmYba53inEyHRtOIfYThp0SlXAKvj/HpFzSXnnllSlpn3nmGT/h87zzzks57uK/ffpoHTbaaKNEw4YNE4MHD05J60JAEm3btk04z60/zmTRbJOAnVKaoG5Myo1PUqV+1POiiy5KluE8xf7YddddlzzGBhNFSetCRVKOx3ect92ne/DBB/2pQsvPxC5eTrb9MAn4m2++SbjVfXx93nrrreQlTKh13nh/7uOPP/bnM00C5iIXp+/TZJoEvNNOOyW4Tzeik3CjJ8ly2HAhN/44592Iks8n2v9SErsdzq255pq+n7gRheTpMAm4GGW5ESOfP+3pRlQSv/32W7IcbYiACIiACIiACNQeAY0AOG2kNgTPdtOmTZNFEZ6DFziEQiRPxDaYNIk45TLlTKNGjbz3FI99PDTFKWspaQmvYYIsEz+jQugPYTYh3tt1O/vss8/86MHiiy8eTWpPPvmkHxVgic18ZOjQoX40gdELPOJRYRQAFi+++GL0sN+Oh8bgjUZycfKJIv9Vt/w4u0iWBW2ST2VlZUp8fvDSF9PrTbhOCANiki1COzIvgJGbdOE88RthXgJ95KOPPvKhZuutt148id9f0LLo84zsHHfcceaMTD/RmZAoiQiIgAiIgAiIQO0SkAFQS7yJ3Y8L8wHCHID4ubD/yy+/+E2UOUI2on8oUwhGQBAMg3RlEb/PmvDMNWCiJ2UTasNk2xDe4jyy3hggVj8uHTt2NP7yFUKIEOL544Lyt8QSS/j5BNFz1B0lMSrUE8nFKXoN29UtPx27eN757DNfg/j3aBgQ4T+EJLmRlHyyyDsNfcN5+pNhQMzNYOJtCCnLlhHpUPgJT3MjRPbf//43W3JvVFS3LPrCvvvua1dffbV9//33Nn78eDvppJOylqeTIiACIiACIiACxSdQWfwslWM6AvGJmOnSpDsWfrzJhYoYCnI6YaJuELfWehWvL0oeq7jgFWfysAv18PMKXNiL9/aHa0P8fojXDser84nBgWTyQOOlRpGMSnUZRfMI29UpPx27kF91PlHMWXGHSbpwf/vtt+2ee+6pTlZZr2ESNJNpmXTM6joYGn369PE/IpbtQgxHjBSWGyUuf//998+W3J+rblnxjJnkzqpDGEgYA8svv3w8ifZFQAREQAREQARqiIAMgBoCW6xsUeRYyYYJt4R6RMXNI/DhHrk88+eff75fe50JxGG1HPJhYu2IESOSyzMGQ2Lw4MHRYvw2SizhGqzgw8hBLqHeSBjBiKZnUjK/bhvSRM8VazvkvbDK5z5YbYfJro899pg3ABo3bmyslFQTQugUv0CNMo9STbnZhAm/jEbQBzBS+PXifKWQspg4jnFBnVh9Kiphpatg5EbPaVsEREAEREAERKDmCCgEqObYFpwzMeMI8fpBwgo7IdwnHCfNuuuu6z2nhPFkE34MCnETRf1n+I84cbzwwVvOUpesFvTcc88ZxkVU3C/r+pWD8JIj1DVaz2hatgkjwqC47777fKhH9DyrIDHawGpDNSULu3zui/kSLKfK/AlYMw8ExjUhIQyIkBraLt3qP6Fcfh+CmH/mgLBOfyHKP3kUUhajVizjSptHhd9agAuMWLFJIgIiIAIiIAIiUHsENAJQe6xzlsSEXOSSSy4xt5KMn7TLZNJNNtkk+TsCe++9t4/bJ54f7zYhJfG4+XhBhP8QfkJ4CAoiShnr5BPzza/ERn9YjOVBiQl3qwHZueee65eDfOCBB/z1rBtP7D5CXakjhgFGSnziKAYCy5yiaLLk5znnnOPnJoRySf+f//wnXtWi7RezfBR3Jsjyx9KVhQjK8rPPPus98yi8NSX9+vXzdWOJWCZOZ/pBNMq/4IIL/AgMaYjHTyeMGmVaqrSQsuiv/EI0ZbIEKPMSiP1n6VZGKwhXYk6IRAREQAREQAREoBYJ1N6CQ/W/pGzLgLrwjCoAXNyzXyoznHAe2YRT9pNLJbJMJ+I8tgm3fn/ChZD45Rxd9/DLKDrlO1zqP1nK0nlUU46xw/VOEU1ey/XuNwUSLDHq1r33x90PTyWvY+lKF0KTTO+U6cTxxx+fcB7/ZBrn0U64Cbo+jftFWX/chZQk3AhBMg0bbu3/hBsJSObljJWEC0NKuJVnUtKxDChLQ8YlLO/pRhLip1L2Q7qwDGg4mW/5mdiRjxtp8fWPL8cZygif0WVAwzHuk3aDFUu3BinWMqAhPz5PP/10X8/o8qocjy8D6kK4ku1BX0j35+LyuTQRXQbUH/jnv3zLIrkLM0u4sKGUctwqUwmWp5WIgAiIgAiIgAjUPoEyinQKgKSECLAsJyvzxJfcJFyHlXpoMiZ9Fuo5xfPKRGB+CCr+I1Dpbn/MmDHG0pJ4vUPoTzQd9SDchBVv8LhnE/KifOYyZJoYnO36BT23sMtf0PrXh+tZbpSJx8T+ZxpdqA/3qXsQAREQAREQgVInIAOg1FtI9RMBERABERABERABERCBIhLQJOAiwlRWIiACIiACIiACIiACIlDqBGQAlHoLqX4iIAIiIAIiIAIiIAIiUEQCMgCKCFNZiYAIiIAIiIAIiIAIiECpE5ABUOotpPqJgAiIgAiIgAiIgAiIQBEJyAAoIkxlJQIiIAIiIAIiIAIiIAKlTkAGQKm3kOonAiIgAiIgAiIgAiIgAkUkIAOgiDCVlQiIgAiIgAiIgAiIgAiUOgEZAKXeQqqfCIiACIiACIiACIiACBSRQPafby1iQfU5qyeeeMI++eQTu+iii2rtNseNG2dz5sxJKa+iosLat2+fckw7IiACIiACIiACIiACIhAlIAMgSqOa2x9//LE9+OCDtWoA/P777zZ16tSUGjdp0kQGQAoR7YiACIiACIiACIiACMQJKAQoTkT7IiACIiACIiACIiACIlCPCWgEoI42bnl5uRHyExWOSURABERABERABERABEQgGwEZANnolPC5VVddtYRrp6qJgAiIgAiIgAiIgAiUKgG5jEu1ZVQvERABERABERABERABEagBAjIAagCqshQBERABERABERABERCBUiUgA6BUW0b1EgEREAEREAEREAEREIEaICADoAagKksREAEREAEREAEREAERKFUCMgBKtWVULxEQAREQAREQAREQARGoAQJaBagGoCpLERABERABERCB0iPwyHtm7/5QtV7n7WPWqmnV4zoiAtUhcNytZolE6pV9Fzc7bPPUYwtzTwbAwqSvskVABERABERABGqNwIQpZn+Nq1rc3LlVj+mICFSXwNCxVa/s3KbqsYV5RCFAC5O+yhYBERABERABERABERCBWiYgA6CWgas4ERABERABERABERABEViYBGQALEz6KlsEREAEREAEREAEREAEapmADIBaBq7iREAEREAEREAEREAERGBhEpABsDDpq2wREAEREAEREAEREAERqGUCMgBqGbiKEwEREAEREAEREAEREIGFSUAGwMKkr7JFQAREQAREQAREQAREoJYJyACoZeAqTgREQAREQAREQAREQAQWJgEZAAuTvsoWAREQAREQAREQAREQgVomoF8CrmXgxSru888/t2nTpqVk17hxY1t99dVTjmlHBERABERABERABERABKIEZABEadSh7Xnz5tnc2G+Xc0wiAiIgAiIgAiIgAiIgAtkIKAQoGx2dEwEREAEREAEREAEREIF6RkAGQD1rUN2OCIiACIiACIiACIiACGQjIAMgGx2dEwEREAEREAEREAEREIF6RkBzAOpog7Zq1coaNWqUUvuGDRum7GtHBERABERABERABERABOIEZADEidSR/aWWWqqO1FTVFAEREAEREAEREAERKCUCCgEqpdZQXURABAoikEgkCkqvxMUjIPbFY1loTmJfKLHipRf74rGM5iSuURq1s60RgNrhrFJEwBM46KCDbNSoUXb44YfbdtttV4XK9ddfby+88ILts88+tueee6ac53cfuO6www6z9dZbL+VcMXZ+/vlne+yxx+yTTz6xXr16+bKWXnrptFn/8MMPdu2116Y916dPHzvppJP8OfK69dZbbdiwYdatWzc75JBDrH///snrZs2a5e/3pZdesokTJ9paa61lRx55pOUKZ/v444/t7rvvtq+++so23nhjO/bYY61Tp07JfNNtLAh7yjn11FPtlVdeSZf1Ah97/fXX7cEHH7Thw4dbx44dbaeddkrbP0JBs2fPtttvv93efPNNHwq4/vrr2957721NmjQJSWzMmDF2zTXX2HfffWdLLrmkbbnllrbJJpskz48YMcKefvppe+ONN6xdu3a+PNLkkrvuusuefPJJo+123HFH3x+zXfPLL7/YCSec4JPcc889vqxoel781H3SpEl200032WKLLeZPF8okmmch2/S9F1980f766y//OyrHH398Csd4Xn///bc9/PDDnhthmNtuu61vr2i6XOwHDx5sN954ow0aNMi3H98JvtstWrSIZlNluz6xnz59ut155532zjvv2JQpU4znBt/9fEe3H3roIXvuuefsvvvu85zyfSYFqPNmT7MRbx1ubZY/zJp2zf08rS32/MbPs88+659tyy67rP9+LbHEEqHaRfssdr/P55mUrvK19Syv7vMuXZ2zHQvtN/T1r6xRm2WttetfDVvm1348F4477ji78MILbfnll08W89577/n3Hc8VnhW77babf6aTIFu/53meTs8IGWsEIJDQpwjUAgEeQijDzzzzTJXS+B2HJ554wisikydPTjnPCxIF9JtvvvGKV8rJIuxQHi/foUOHeuWdOSYozOynExRNXkrRPxTwDz74wHixIyg3RxxxhFfM//Of/1jXrl39PvcQBIUQowfFdL/99vNK1YknnhhOp/3kRUN+ffv29S9/DBcUiVxSXfY//fSTUScevjUhKMinnHKKZ/nvf//b39fZZ5/tDYJM5Z155pn+hbD22mt7Q/HVV1+1//73vxa8aLTnHnvsYePGjbOTTz7ZevTo4fsPLyeEdCjl7B911FG26qqr+usxCLIJBt0NN9xgZ511lp1xxhl++4svvsh2ie+vKNf0+9dee61KWq7//vvvfb+fM2eOP18dJlUyzuPA22+/7e978cUXNxR/6sILOJtcfPHF9vjjj/uXMC9Y9h955JHkJfmwx0AePXq0Z48RhbFD+aH9kplFNuobexRqnnebb76574szZszwz4BMz5wICqN/XHbZZd5gDsfzeSaFtPNmT7HRH5xqM8d9Y4l5s8LhjJ+1xf6PP/6wQw891Fq2bOmfCTzr9t9/f+PZVUypiX6f65mUrv619Syv7vMuXZ2zHYu2X7uVT3F9a7YNf21/mzM1d/vx7OO99ueff9rMmTOTxeDkOfroo61Zs2b+2YQuwHuV5weSrd/zncoqDoxkAQk4xSzhvFYLmIsuXxQIbLHFFomDDz44sdJKKyXGjh2bcsvOE5LYdNNNE86jnXCe0OS5999/3x93ikJixRVXTDglKnku3YZT5BPOO5yYMGFCutNpj11yySUJ8o+K8zIknHITPZR1+5xzzkk4hSjhHlA+3ZVXXunr7R5sft/9cF3CvewTF1xwgd8n3TrrrJNwow7JfLk37tE93JLH4htOafVpnLfEn3KKQMIpu/FkVfarw96NciT69evn2ay88spV8sx1wBlDCacsJnbdddeE85inTe4U6YQbGUk5969//SvhPDcpx8KOe8n4+3dGTziUcAqCP+ZGj/wxN+qScJ7pxNSpU5NpnFcpcf755/t9ODulP+GMmuR5nmNO2Ujup9sgD+4lyIABAxJ33HFH2E376UYgfN3o986orJLmvPPO8+1Hu//+++/+fKFMnELo83YjNAmnVFQpI90BZ3D7/ukMmuTpIUOG+Lp++eWXyWPRDTcS5M//+uuvycO33XZbAg6hn+di7zzeiVVWWSXl+0+9uX9nzCbzjW+UKvt4PdnP1e8570atEo8++mjycvqqGwFMOIdA8li6DaccJXbeeWf/t9FGG6VLkjwWfyZx4j9Xv59Ypf+miVXXnf883eyo1xI7uEfSuMnJy6ps1BZ7voPOAEiW70Z9PRM3ypE8Ft0olX6fzzMpWu+wXVvP8uo+70I9w+enn36acMZ7wo0YhkMpn9H2o09td860xEr91kpsdMB9vo+d90hK8pQd3jX0a54DcAmyyy67JFw0QNj1ny5CIOEcOCnHojuh3/M9yyYaAchqHumkCBSfgFP+rUOHDlW8oQzJ4g0rL0/9WhISRJhBGOrOVSNnRPjQjs0228x7NxkizCV4YN3LNCXZhhtuaM74SDmWaefDDz/03jynYHpPBem4R8IlghcCD4d7IFnbtm19Nng0CKnZfvvtk9ni/auoqKiywlUygdtYZpllfB5XXHGF99gyXA6ffKQQ9vzSNvXDw4tXrhDB4039aANGODbYYAN/X+ny2Gabbbx3PnqudevWGUd6fvvtN5/UKZ3JSzp37uy50A4IfYaymzZtauEXwv/v//7Pe5g4z4gLoT/t27dn1wvsSZ9NnMFmjIgwguUUX3NGpi8n2zXhHH2bEQdGJYLQJxi9iIceFcqE+yd0ZODAgT4vws6i5YTyop+cx4tGCFkQwo8Il8rU7wmnYjSFNEH4nowcOdKc8eIP5WJPGYTPEXYVJHwnop6/cC58lir7UD8+8+33eH4ZRXJGefJywqnwZobnRfJEbIOwNp4tjJxkk3TPJNL/8uUL1mKJba3bgPmhQ9nyCOdqi71zjphznIRi/agjz8zmzZsnj0U3SqXf5/NMitY7bNfWs7y6zzvqSeitM1TNKec+TK9Bgwa2wgorhFtI+Yy339zpYywxd7qVV6Zvv3Cxczj4sEK+E1Hh2c1zJfqs5zzPnPCsj6ZnO9rvGzduHD+dsl+esqcdERCBGidQVlbmFX2UyyAoQs5LUUUR4jzhIIQlxJd9DdfGP4kfJ5QDxZOXx7777uvnFDz//PPGizedEJ7BSzUqKIdulCJ6KO02daeOvJCdZzOZZuutt/bhLIQSEe/MJ8oP9QuCwllZWenjGC+//HIf1048P0PgmYRrTjvtNHvrrbe8Yk4Iy1577ZUpecrxQthjiDz11FOGQZWPOE+LD4Gi/hgkhEAxLI5hxxBu3LALeRLTSfxzEIZ4USLXXXfdcCjlM7CJhkoQj++818n2QiGlPZ1HyueDAYLSSVsFCflQFmFChBnAMpsw9wQehBsRBnPvvfda9+7ds12SPIfxxfwG+nkQQitoT+f1Cof8Z6FMUJDoE+RNKBufvDQZUo+GnEULoc8jUSOIfRTzTP2eORrx9GE/XJOLfc+ePQ2FMirMKYBNpjk3pC1V9tXp98x1wPCKKraEOsAw2/wm4sUx+Hne8F3OJJmeSaTfaNezre2Kx1lZeaNMl1c5XlvseebgGBk/frwRIkWo2Oqrr57xGVQq/T48S7I9k6pAdQdq61lO2aGO+T7veJ66kXH/HCHElPcZzoqLLroo4zMv2n4TB91lo947yZp0XN2aLZb5HeJGvuz000/3z68uXbqkYOKdwXcFwzoqbgTSv9u5NirZ+n00XdiuNQOAlwtf2OgfllSw5HkRFFtQmHjpLajQqExOk4hAsQignES9oR999JF/QEUn/oSy6H/VkTXWWMPHyb788sveY44yFDU6Qp54uolvDw/IcJwHD8poJqMhpHv33Xe98hiftMxE3t69e/t4XSYxMRKBlzb60g95oCSThrI4j1KRSb7++mu76qqr/KRiHnhBAcvmPY3mVQj7TEp7NL+wjRcM5ZNJ3ijHeKHxwvOcy1cw2IgFh0GmWHT6CPdM/sSL8hK4+eabvaeK69nnj1h9lInrrrvO3DCyV9aZOBwV+KF8uqHt5MhM9Hx0mzZh4jXtjecxyh4DJJekM75od7zA2ZS5fJiEsvEgY2ByT/D57LPPvAGcrg+jqCPp+j0jG+mEa5gfE5UwcZdrCmEf8kDR43vJ6EWmvlLK7IvR73/88Ud//7QdCm86YW4F805QljCWskmmZxLXlJcX9jxdGOx5VjIvBO8vc6v4rmWTmu73g4eOtL/ntLIzHzA74z6ze940mzBj/oR1+n2uZ1Kmutfms5w6FPK84/nLHBX6G5PNcWCFkbpM9xOO035T/3rdZk3+3SqadnJzATK3nwsvMyZ7M+qZTvD2h/cj72rek0ycR+KjZdn6fbq8a80ACIUfcMAB9sADD/g/FztqDEuj/DA8zAQKiQgsCgTweGLtB29oUISKee88HFit5ZhjjvHDgjvssIOf7BkvAwODocL4wwTFC+Umk1IS8kHZZRJp1IvNOcKBeFBxb/fff79hiDDUyUs8Lni6qOull15qLibce8DjadhH4T3wwAO9V4YwFFa/IT+eHax2kI+hXlPsmeTM8w1nB8zxGmVSJNPdGyEpLk7eT2xEeQ2KZTwtXjO8UChe3DOe1G+//daPHmE4MKKCUB+GlN0cBr9KEsPgrJwSFdKi1NMP3RwHX3+U2HRCOgyNq6++2t8bSj8jDEycZfUmJmPnkhAGhKcXpZwwpHj4TzSPfJlEr4E/YSKMaiCMyAQm0XQYR0i837Pfpk2baNLkNnz5XkQl7HNNKCcf9iiWhHtQV/p8fFQgWkYps1/Qfs/7H+WKCe0oW5nk3HPPtdVWW61KOES69JmeSenS5jq2MNgz+sfoWnhmMpKYS2qq33/l1LLvhzW3YaOn27d/uu2hZk9/bHb8zfO/B/T7XM+kdHVfGM/yQp53hH0yusloE99PQh/zFdqv22b3Wo/tXrYZY760MZ+kbz9GFAjZYUQ1k7BYA+9WRpF5RvC+4z2BhGdYuLbQfj//TRGuroVPrPu4p5AXGC8pKs9qGBIRWBQI4InmAU/oDIoQD/xiCA9WViUhDIgH8+677+5DcDIpNZSJR41lOKPCsoy83LMJ3m7ipYmTjwqxiyj/KMTBW8doH8uXoUCiPDJCQLqol52hduJCuZYh17gwfIsXhoci1+E9IfwHBYJhczzu+UhNsIc1qwUxCkA9cXQQhoWHm/uOh7lE68nLm+VdeaDTD8KoRjRNdJvnKCFdhBnBlbh0VoYgxIqRT7zU8VAKlHTqlY49bUFbsQwsChnGQlzoTxgc5IOwCgtLV6Jos4pOPss3EjtLn8K7iWeTvsHLDU9gXAphgjLNClR4/vGCYcww4sW8lqCUx/MP/ZJ+HjW2+B7wPkonsI4rAlyPsMxtvuzx5GGcwYFR6mzKP3mXMvsF6feMfLDyFToB/SjTSBAhEKSFMbHYCO3EH/sYeXhKkUzPJH8yy3/v/2jWPE3I9AOPPG391tnOZjRfy35y01f2Puoyu/Giw+3Y40+29p0Wt2Ezl7Jh31bNeMj8KSH2yS/OGF9sBWvTvqvdeM/r1qptJ2vWsqP9NaOP/fHb/H7/sbOdB0+Zn0fqM7GDrbTWjvbyU7fYq1/Odn05dTSRfv/jtx/Y+689bN9/+a71Wmpl225vt0Laahu5OUeV9tZ3Ves1fNL80ZOXPplk7TvO9+KTasiIida6az97M3IvI9xA2KPvu1XDGnawOTP/pwDPcwO0c2fN7/fv/tbNsXEZNF7dTrvkeRs6eJC1at3BOnTuYTdefIQrY7GUPEONXnz8BWvWoq2ttMkJ9u6gcttqnwvtx1/2sn32O8imTB5vrRbfLO11XD/DDTj+6RbBoa7LrDLAHn7CjXL32dFeee0NO/Gse/3xmS7a9Y9R89NwTSrXhrbMWvOfd7c9/JGtuHrV513THlvb0Wdtbb8O+szeefkB9x7dwxbv3dfW3XR3W7n/gCptQRlItJzKJh2sZa8dbcL3t7i5AK5Cltp+OMFxhASFPoz0oORjRPAMYz4YI7esfIVzgtEWnF8846Mx/tXp97VuAHhCsf+WW24572UkrikIwxx4A1h/mwc1Q4N4AKLrg+eTJuTHJ54tt7qGX/OZ0AYeunhfeHkGQXnC4uNFycMGT5tEBGqCAIoh3iViWlGKohMLF6Q8lGOUfRQMjOt8QogYgiRcAkU6CPsodtmENDzw4vHqQannBRUVlCUecpwnNhtvPpNJw9wBHoZMzKQ+6YTr8LiixPLwwyPLdxQFgudE9Luc7vpwrKbYkz/14nnFH3xY35+l/AhzTDeagqLLeRRoYk5RqLIJ947nmLAeRl4QwiMIKQvKEUZUmJgX8kLJZtlWnqGEUGF88iIJdQpxpoQTpBPYB2WX8xghLN3KSEe+hhfXMQqA5wtlmnZIJ4Uy4V4xvrbaais/ypEtlj6Uh3cPg4s2CvMY4IiCT79MJ3DlfkkXQoe4HoYhfjcXe/Jl5IR3G9//fAynUmYfOBXa7/ntBRQdnH5xp2DIM3yiBMW9pIStYexxLfMqgmR6JoXzmT7veC39mQlTyu3LnybZsOfC+dWtxdL72aRBd1p5x83smuTxcH7+58zx8z8feNusYWunGLbf3J578VVDKWzQdgt/3Yx/pljd+5ZL46Y9DXlua/ebBOtb+36nJTMb89lfNjdRade/WOkMpORhvzFr0m827JUTrfniW1nXAQ9Zos3S9o5brfidF1PTRffmzeluZZXN7OaHPrMWvbr7U3NnTXaK+082s8uBNijN/TRqs4xN/PFOp/RPtgoq6mTG6M9cPFUD+2BwF/c3y8Z9daW17L2LNWw1/5lE2iHff24TWu6cltGEX8ptytTpds0zs9wkWSyv5tag70U25ZU9raJJR3v48x5W9pUvqsp/k93gA4bVr66uM2dvYcN+vNsuuuFZZ6h0tUe+WtLMXTfFrYD5kbNZfnJpxn19lU0d8rIttvUzbu7HfCV82si/fL4vfNXE3nKGQmZZzaz3arZYlxE24deH7YHbL7aXvzRr0bOqgypd+82e8pcr06na/MUExT/qeEMv5dnOPCtCeBEc4zyzowtdMF8mnA9ZVqff13oIUKhs9DNMTiReGCFMYENnzfOAJT6SoQ+G9nmhBsknTUjLJ4oIUJnNzY/OMCmR4WVeYmGYnhcrL1UeLBgGeMTwLqLgxIVYUIDzRz4SESiUAEouigdf+GxhEIXmi9cZzwJKWT7KP/nzncDzSygI3xU8k3wP8EoH4TvICzcqxKhiKKdTWolpZJ1/QkRC7CIeZpQ+vLIoSsSSMyLA5DHCQlg5hwdiphU+WDEIQ37gwIF+0iuhL6y8gILA9SiBfI9zSU2xj5dLyAKTm1F2MrUFoVLMX0B5hxWODf7waAeJsudlgLcHbrBCKcfY41kWJixjyNGGxLBiVPHCwIMa4kyZFIySTUgPPOFIv0GRyuT9pk3oH6xGxTOPe2KkiUmzeN6ja+GHeqf7DGFA1ClTv8+HSTRvDOgQR5+P8s+19FmY811hQjBtwHeR91B0Rawoe44zckE/JT2TgjFgCRcIhlQu9hhdcOQ7h7Eb2pvPTO+SUmYfbYewnavf810lrILRJEatogxCKJlb5tePjtBPMfR5N0f/mFSO0cEx+m2QbM+kkKaQzxbOgzt16Gs26af7bO6McTblzxdt8i+PWEXjdjbZKYSTf30kr+ya9djchYN8blOHvWnNe2yZ9ppm3Ta0v397wqaP/NjmzZ3py5oy5EVrseTOaUdHKpt2dWEmr1iHNQa6H51aOm2e8YPllU2tpctv4iDnfZ4yzJcz3invDVv2tmbdNvLJZ00ebOO/cYsG/LN+fVN3vLJJJxv/1RU+/eypw23iD7dZm+UOtbKKBu6voc2ZPsrGf321zZ050f1NsjEfn2WNmQDbfdN4Ffx+iyW2t3mzp9rYTwfa7L+H2Ixx39rYzy+wcmctzXWW0aj3TnRe89zP8kZtl7XK5t1d3a50hlAGrl038Pcy4bubfZmUNeGb661Bi57WuEP60b54pSubdbF2K51gPbZ/1Zp2XjN+2u/n035893nmIDyvo306PJ/Rf4NTDScFI+yMAPAsR4fFeRBCHH1G7r/q9PuqJknIrYY+8VLxskB4UVNplHFe4CjcCC9xvtw8KMOQIJ4rYpgJlcCrmU8an9k//wENBYfJHCG0AG8RXihe0DyMMDYAS514KCGEG/CrrHFhdRBeFkFCPcO+PkUgHwIoQygQmTyh+eQRT5MrbCeenn3CUxgdY8gRgxclh1VVonH9fD9QRKKhJXxXguEez5fJrHj78XLgwcQIwIvBvB8ERRZDm5E+HnwoyIxcMA+A7386wWDA44/SH5RqVozBsGApUzyKKA75eFVrgn26OnOMeqcTFMhgVDFyExWMJJ6XSJw9IQ9wIpQJtmuuuaYfIQ3PobDPiALKNKxxoPBLzAgjLm6taJ8HHm3aCaWNkCqUqnTi1v/3czBQfimbFxMOEsKAaBOMDRTqTIZOyBPDj+crSl14zoZzfObLJHoNynw6IzSaJt02YVOUxzsBRoQo8S6Ihg1F2XNvvC/wWhOzDiveR9GRs1zsw/sPAy4uHOPlH5dSZh+va3Q/U7/nPYy3k74f+n+4jrbg+82IFM9GnjdhdCWkyfaZ7ZmU7bpM51r03tUpqH86D/cVNu7LS733vNVSe1mbFQ53CutFNtkp7C16OQU9x+RivOgNmi/mnObO091ivn4RL7Ptisd7BXXEW87xUjZfPaOstn2PjSf1++UNGC3MPmKY7sI2KxzhFOLhNtSNOKC8N2q7gnVc87z5nmp3wRzntUbBb9plPUPx5d46rXu5jXr/FBv8+NpWXtHYmnbf2Fovd1Ay+7YrHmvjvrjUhjwzwNW93Jp0WtM6rHluUodLJvxno7JZZ+u41kVe6ceochVx1/S37ls+ZjPHf29jPjrDZv092Bq1Xip+aZX95s64or7NeqQfUWzcYRXr0P8c334T3ciNJeY442Q167zWhf+MPlTJMuOB8gq3ehR/aSSf9uNH2Ij7jz4z0mSVPBQWreA9yug372Se0/HvRHX6fZnz9qWO0SeLLe4GVnw6xYSXCC9whqT5ZU+qw4uBcB8mBkaFmCcevLzkc6VhuJB4TOJUeVkyKQ/PFV6i6AsKJQXPDd4olsBjZCG6vioWFw95vGO8LILgqcF7g1B3lBGWjZKIQF0mgCLIikDxh8uC3BOGPiNmhOhkUi7DpFAUhqDEZiuT5wTPFAyGaMgKvzsQjefOlkd9OEcoCoprJq6BE+yjSm24d4wyvN84YEJISziX6ZP25NnH8xzjI0hdZh9Wu8o2TybcZ/hkFAYHUfD8h+PhMxf7kK6Qz/rIvpD7L0bam18ye8mFcBQqeKPnTB/tFOKu7hn1v34/b9bfzmv9v1j6QvNNl37e7Gm+rAbNunkPe7o0xTjGLyLza7UVjdrknd2caaNc+rYZ60XoT1l5w7wVa74nc6aN8HUor2ySrEdNcE3Mm+sMn2FW3qh1MpQpWWARNzK132q9zU7ftXoF8W5mtDf6+yHVy+l/V9X6CADeFeJdERR0PPBR4aHKDy8QVhAXjuEVyCdN/Fq8gsDD2IhLGD5k6BHPZ1R4uMfryHkaITQEL0+JCNQHAiiJxVT+YYKCmis2P9ek1zhbjIR0DoVFSfmHSS6lPROnwBNnSK62CWnDJ+0ZYubDMT7rMnscSoVK/F0Rvz4X+3j6fPbrI/t87rsU0uApb+BCTeJSbOWf/PHsN2zQM15U0fcZjShUKt2yltkkzBHIliZ6ju9JA2dUxaUmuDKS0aDF/+Z8xsss1n5NtB/v5qBzFquetW4A4GFJ9/IIN8R5OkSIyw/H+WSVj169enmvX6400evYRknv6eIE+SVHro1K2KdeGApxybQsXjyd9kVABERABERABOoegSNd+HiL/zmg694N1ECNh7uJzPe9lT3j3dc1Wzz7zzJkz6Cenr3kidK/sVo3AHIhYVSA1VDeeuutlJAbflgBzz+xyvmkiZfD0kmsRU4+TJZDGHpikiOhR/zoA0vHEdsfHcomPcPDpSaMksQnJ2PIEFIlEQEREAEREAERyJ/Aam7xmLbFjeTJv/ASTrlyT7NT7nIjuRVu3ubc+RWtdBFQjdxiOoe7kPv1livhyqtqWQn8L5Ata7LaPclERCbtMjkYRZfQHJZlQ1EP6yXnkyZaayaqMbpAzD/zAIjhZ7lPZmMzmQthOTGGsTEKOM+8gH333TeaTclsM+ExrEIUPhndkIiACIiACIiACIhAMQgs2cXstmOcfrS+2RpuPu7qvc12X8/s0gOl/BeD78LMo+RGAIBxgFsNiBAgViFhCVA8/vziJ6sChZjXfNJEwTJZi7WnWQWD1T/wlrMSxg033OBXvyAtMZ384ApLuhH3T3ws6zWHn4yP5qdtERABERABERABEajvBNq5kZEd3MqXO9T3G13E7q/WVgGqDldCXPhhLib7YQSkk3zSxK/DuGAN50xLlJGecCNGA/JZWg5DhR/7qc1VgFijPT43gdVQ0k1yjt+/9kVABERABERgUSSQaRWg252XWyFAi2KPqJl73vHCqvkuyCpAVXNb8CMlOQIQbosl5lj/P5vkkyZ+fT5LveVa4SGep/ZFQAREQAREQAREQAREoC4QKMk5AHUBnOooAiIgAiIgAiIgAiIgAnWRQEmPANRFoLVVZ5Ys5UdhopLuh36i57UtAiIgAiIgAiIgAiIgAjIA6mgfKPaPNdVRDKq2CIiACIiACIiACIhAgQQUAlQgMCUXAREQAREQAREQAREQgbpMQAZAXW491V0EREAEREAEREAEREAECiQgA6BAYEouAiIgAiIgAiIgAiIgAnWZgAyAutx6qrsIiIAIiIAIiIAIiIAIFEhABkCBwJRcBERABERABERABERABOoyARkAdbn1VHcREAEREAEREAEREAERKJCADIACgSm5CIiACIiACIiACIiACNRlAjIA6nLrqe4iIAIiIAIiIAIiIAIiUCABGQAFAlNyERABERABERABERABEajLBGQA1OXWU91FQAREQAREQAREQAREoEACMgAKBKbkIiACIiACIiACIiACIlCXCcgAqMutp7qLgAiIgAiIgAiIgAiIQIEEZAAUCEzJRUAEREAEREAEREAERKAuE5ABUJdbT3UXAREQAREQAREQAREQgQIJyAAoEJiSi4AIiIAIiIAIiIAIiEBdJlBZlyu/KNd91KhRNnv27BQElZWV1rlz55Rj2hEBERABERABERABERCBKAEZAFEadWh7yJAhNnXq1JQaN2nSRAZAChHtiIAIiIAIiIAIiIAIxAnIAIgT0b4IiIAIiIAIiEC9JNCxtVmfLlVvrbKi6jEdEYHqEujT1V2ZSL26a9vU/YW9JwNgYbeAyhcBERABERABEagVAjuuacafRARqksAl+9dk7sXJW5OAi8NRuYiACIiACIiACIiACIhAnSAgA6BONJMqKQIiIAIiIAIiIAIiIALFIaAQoOJwrPVclllmGZs7d25KuRUVCmJMAaIdERABERABERABERCBKgRkAFRBUjcOtGjRom5UVLUUAREQAREQAREQAREoKQIKASqp5lBlREAEREAEREAEREAERKBmCWgEoGb5KncREAEREAEREAERKGkCZ313if0+9c+UOrZu0Mqu7XdByrF0O88Nf8UeHvp0lVNnLney9WnRq8pxHSgNAjIASqMdVAsREAEREAEREAERWCgEPhn/hX058buUsjs2ap+yn2nnj6lD7dVRb1c5fXyfQ6sc04HSIaAQoNJpC9VEBERABERABERABERABGqcgAyAGkesAkRABERABERABERABESgdAjIACidtlBNREAEREAEREAEREAERKDGCcgAqHHEKkAEREAEREAEREAEREAESoeADIDSaQvVRAREQAREQAREQAREQARqnIAMgBpHrAJEQAREQAREQAREQAREoHQIyAAonbZQTURABERABERABERABESgxgnIAKhxxCpABERABERABERABERABEqHgAyA0mkL1UQEREAEREAEREAEREAEapyADIAaR6wCREAEREAEREAEREAERKB0CMgAKJ22UE1EQAREQAREQAREQAREoMYJyACoccQqQAREQAREQAREQAREQARKh0Bl6VRFNSmEwG+//WYzZsxIuaRhw4bWp0+flGPaEQEREAEREAEREAEREIEogawGwNFHH22DBg2Kps+4veGGG9qZZ56Z8bxOFJfA+PHjberUqSmZNmnSJGVfOyIgAiIgAiIgAiIgAiIQJ5A1BKhNmzbWqVOn5N8XX3xhn376qbVr186WW245mz59ur3zzjv2zTffWI8ePeJ5a18EREAEREAEREAEREAERKDECGQdATjvvPOS1b3//vvtpZdeMkJPOnbsmDyOJ3qVVVaxyZMnJ49pQwREQAREQAREQAREQAREoDQJZB0BiFYZA4CQoKjyz/m2bdvacccdZ48++mg0ubZFQAREQAREQARKlEAikSjRmqlaEFD7qB/UNIGsIwDRwlu1amU//PBD9FBy+8MPP7TOnTsn97UhAiIgAiIgAsUkcMcdd9jjjz/uw08vvfTSKlkTivrvf//bFl98cbvhhhuqnL/mmmv8SPUZZ5xR5VyxDkybNs0OP/xwO+yww2y99dbLmu3PP/9sjz32mH3yySfWq1cvf93SSy+dvGbkyJF21VVX+fcuiztsscUWttlmmyXPs3H++efbsGHDUo6tttpqdtBBB6Uci+6MHj3abrnlFj+iv9RSS9khhxxia6+9djRJle3qsCdE+M477/RhwlOmTPELVBx55JFGmbUtr7/+uj344IM2fPhw78TcaaedbLvttsurGoMHD/ZOzgsvvNCWX355f829995rH3zwQdrrd999d2NOZFQK6Xsff/yx3X333fbVV1/ZxhtvbMcee6wPw47mF9+mvUeNGuX7ULr7uv766+2FF16wffbZx/bcc09/+YgRI+zpp5+2N954w4d1T19qktmS8ZxrZ//zzz+3Z5991t/zsssu678/SyyxRMbCudezzz67ynm+dyuvvLI/nuu78eKLL/oyo5mUl5fbddddFz1Ur7fzHgHYfvvt7amnnrIrrrjCP3Bmz55tf/31l11yySX2xBNP2G677VavQZXaza2++ur+IcODJvz179+/1Kqp+oiACIhAUQhMnDjRv3tee+01GzduXJU8n3/+ef9OQjmIywMPPGC33367/f333/FTRdtHyT311FP9nLhZs2ZlzZeQWZThoUOH2kknnWQ42FDi2Ec4j6LGIhwcX2aZZYyQ3JdffjmZ79y5c70C16hRI+vZs2fyr0OHDsk06TZQ/gnlve+++3y+Z511ls2ZMydd0uSx6rC/6667vG6w+eab2wknnOBXrdtvv/2S95jMvIY3fvnlFzvllFMMhRIDsW/fvl55xCDIJXD5z3/+Y3/++afNnDkzmbx9+/Y+P/IMf6zKh1HQtGnTZDo2Cul76FWURx1pH4xEjKhcgjKPIfjMM89USTpv3jzfDuhrIVSb0QXaBMX7qKOOslVXXdUm3DfCGn9RVuX6mj7wxx9/2KGHHmotW7b07QSD/fff37inTIIzGkMp2u/ZDuzz+W4wf5VnRTyPTGXWx+N5jwDsscce/qF74okn2sknn5xk0aBBA7v11ltt1113TR7ThgiIgAiIgAgUm8Biiy3mFUmMADytQXjhv/LKK350IKp84+3+73//a19//bV16dIlJM/4ibKFsofyHfXGZ7zgnxMofijSLVq0yJXUn7/55pu90n/jjTf6/fXXX997+lFKMSIefvhhY34dSmC3bt18msaNG/uV9tZZZx1r3ry54ZlGKeV93L1797zKJREe37322ssrPltuuaUR3stoQ648CmGPMvzQQw957/Uuu+zi68Yow6abbuq9riid+QhK4COPPGITJkywgQMH5nNJlTT33HOPMSpy+umn+3OMzKD4Ub/gDa9y0T8HbrrpJov2p5AObvwFYUW+nXfe2TtC11hjDX+40L7HRd9++62NHTvWttpqKz+ShVPvs88+C8Vk/eQeSYtxzEItQTiGZxujJQh9ACOQeZ0cx4l42wd326zPJ9mMfvmHhpE3fXZuX6dKtg25F/YJY+qOIYxgjGyyySa+n+69995pM/vxxx/9iBLflXSSz3eDPBgtOfDAA9NlsUgcy9sAgAZzADAEGJr69ddf/VBev379/INskaClmxQBERABEVhoBMrKynwYDMp+1ABgdTp+B4Xhf0JqgrByHR5yFGrCaXIJChdeUfImL953KCM4urIJ4RXbbrutD6dZa621siX1577//nvbaKONUtKhhKGQIbxfV1hhhaTyzzHOX3bZZf79u+6669pPP/3kvaYo7mPGjPH3iRc1l6CIY1iQx9VXX+0X8cil/JNnIezx4mIQBWWY62kHlqqO/34N5+KCdxfF8u233/ZhN4QpIddee61X3uPp2ScKYcUVV6xyaptttvFzFaMnWrdunVaxj6b58ssvfb8hnIzQmWxCu9D/ghJL2kL7Htcw0sO8SiIt8IITFkNYSz6y0koreeM1bhzTpxiFiY4eBQU72l/mjJlliUa5SyLMjdE2DCgUbcLS2vXuaDberNlrZVbhIomC3PbuLfZUo/mWQab2ueCCC1L6BH2Z8DGM3ExC3ycci5EMRjYIQY9+R3N9NzCcqXtYzRJjOxjamcqsj8cLMgAAgGVJ5+GvvgtDZ1jOEhEQAREQgdIgQCw8YRVRT2dQcuKhLCgnpM9XUAiIASacgvj8iy++2JhvgBebUe6oFzWaJ/HIFRUVxkhEPkL+8XqRN95fBMWMeHXup7Jy/msaowBBQULwYHJu33339WFHHCNmnLpEFTuORwWv6ZtvvumNHDzy5557bvR01u182TMSQl2iQpncX6a5ESiWKLwo/oxI4GGnnYkJD8L9Bh7hWPjEQEkn8dBYQrUw2KIe/Ph1ePQZMTjttNNyjhzhECUMmtCq6G/xFNr3qAMhLJTJ33vvvec/8w2v5v5R9KPGMf0Hg4CRpqgBQFmhj8CCORJzJ8y2KXvN41RaGTJkiFf6CTPiWr4TO+ywgzdYrv3ldm8AGOpSRGWqqKxItlem9uF706xZMz/iRd7PPfecEeJM38wk9H2Wqac/0lcwvpgrQYgZkuu7QVgY+h1hahh6GBwYwczzSGdEZqpHXT+e1QBg6A24+QiTlLJ9ofLJoxTSEFtGvCBfPob+8MLQKfgyS0RABERABBYuAeKj8fgFTyfeZrYJRUWBjArKRXUEb+Dxxx/v46NRSK+88kqvVOAxTyeFlIORgBIfFLCQH0ozyin3g7ef9y/3hFKD4kwoCxI86ITHEBZEWG7v3r3t1Vdf9fVk8iOGSzqZNGmSD4lCUcULiuKE0sm7ju1cUgj7aF4oZAMHDjQm36LcpRMmvhIOwugLyly6cCrmTSyIoOjRrniXWb0wk/DOx/Bg9CAYZZnSMrrEJO64oVFInwh5E6rGSBX9DyMxGJy0FSMouWTAgAF+dCcYxx999JHvZ2Hycvx6DATqTxx+eXPUwcxzV4445gg/+tLngL627LorWO/Oy3olPJrn1I1Tw4cOXPdgW63t/Em50XTpttG9MER+//13zz5uzIdrMBSZy4BuxsgLijzZRkElAAAw90lEQVQjWpdffrnnhnM613eDuTbw5HvGPFbKxvCnbzDXlTk5i4JkNQCYuc7Qaj5C/FtdNwAYBkLR79q1q7fm+fLBgHg8vkjEpklEQAREQAQWHgE8iSg6wdNJ/D1hEyhscQNgQWpJfDPKEfHShFdEQ44WJF8UQxT3oMiHvFBOUXoJZSDOn9WEmCuAlxKjAIUVzyXeUiS+EhLvYBRtluQ+55xz0iqMRxxxhA/lIXzj/fff93MKUOpRNnnHscpSNqkOe/IlNIaQoxCHn64MFDfuj5EXFFJ4o6BFPf60NUZSOuE+ss3zQCnGsGA+AYZVOgODfDGkWNkwFwvSYlBhfEbnRXK8OsLcE+LRCTeirTHsWLGKORq024477ug/s+WN9xoGwThmZCw+0hS9HrYYXhiAG5ywmbW9bYaN+fe8KqFAp397of3Rf4yVvz3Hfrj9C/visy/s/rUftobdmthXA95IZtnQDVKVzfjfSMyncz6x8c1H+/O52of+wR/GMRzOPPNMH6KWzPyfDQxW+kFUGNWi3RhBoB/l+m6gq0b1VYw3oj0INcP5u/XWW0ezr7fbWQ0AQCxKguXHsCufwUtBx8Ag4CEsA2BR6g26VxEQgVIlgFKDRxzvLEpO9GW+IHUm9IMQBBR/vIQ4f1DAiM0upvB7OqysExWUSd41QZgoi1JPrDLGDco3IxHZltxmMiUjBziz4sowkz7xdDLRGecWK/thMOA5ZUQg04TLUJ/wWQh7jDRW3mGyLUpypjAQ8mZ5UGLf8d7CHyOGkQwY8MfqRqzkhIc4naAsx+85pCNPYukxnljCM3jWw/noJ0ueYnAdfPDB/nDwRJM/Ciqr9AShr6BEM/9jQYVQHAxZVudBGWUUgsnarAIVnJP5lIFxTLgPBgPGK/ebTqIhzoz+NN+knU17f6I1/M1s5nLzr5g9b7Yd+Mnx9uLI1836umPur8EfLlTpo3JrdO1Mm9N9pq3zzQBbe+N1/AXN3i63ivkRan7/gVfutcqy+WpmtvaJ1oV2pu6EVNEO0dj++bWq+j9GNT9Imy1iJdt3gxyZ80M+IcSuain170hWAyDd7dIgWOnMouehlO1hlO76Uj5GB2LCT1D+qSteAh4W8Yd1Kd+H6iYCIiAC9ZkAsfqESaD8M1GUWPFiCEYFDiDirlE6mSxaE8K7kxEGlLsg7PMbBgiThLk3POedOnXyxxjdQFEn9AGhfkyyJV48CF590oRrwnE+w3w2DI0gKOVcg/c5+t4L59N95sueddZR+gipzbXaTrQclHiUYLzeKMWEYLEcKuFXGACFCso/E2oxMAj3CEtFZsoHxT/6vmfEAcOLmPTopGauZ8I5imUYlcmUZz7HaR9GgfDGM0JEmNJFF13k2WEw9ujRI59svMcfrz79BYNyySWXrHIdYUYYCXjMg4I9Z9z88J9EJBJswuxJ85X/SA6zl3AjH0u4+ZHOfm36SZlNe2KsPTnzKbOVzCYcPC+S0uyuda/NGQKEt51VsKL9mIm9GFbR0Z+QMb/3wUgOhivsEfRS2mKDDTbw+7m+G0wmZ8SAUZ5w/0w8JzyPcK5FRcoLuVEejD179vSzr5ngwxeVDsaDqj4IBkA8xpChJjwwa665ZsotEofGMl/8xYdyUxJqRwREQAREoOgEmPBIzDjLU/JeKoYQo47SiRJYTOWfdwhKB8oogred0BhCNVjJhNhnwm3Dii/cE8oJnnA80CjAhKMS6hAUWEakScM7incQP+qEUoOyG5T9KBPWqyeUiXwY3cdbilLN5E4mVJJ3Ju96NB+2c7FnZIbfLWBFJO6F8sIfa9vnIyjBtAchTVHlMJ9ro2mYE0EMPUohK/OEekTDSGgHPP8Iij4TXMMf8wCQDV04EiMAUYFXOgU7mibfbUZkGIEaOHCgbxOWBGWFHPohPAkJwjjIJRiXTGjFaMk0MoaiTF8kxIwyKevv58bYnPYJmzXfBs1VjM1ztvGUAQkb8695NnPJRM70mRLAlUnUKOC0E4Yjf7RXGDHie/7uu+/6LDDk6Bt8nzBc0cEI+cFQC5OAc3030Oe4lsnRXMdEbraZS0P43aIieY8AsDQaqyAwg58vIxYbnRKPCTPBGWrK9WuCdQ0qXwyWPmWCc1iGLNwD980XLEjoqGFfnyIgAiIgAjVHACUUj3AmJac6JafznFcnn/g1eDRvu+02//7EcUas9r/+9S8fTkL4A+XyXuVdgzBBmHh57g/lhtAQFEQU4iCMDuCIwmlF6AKeTBQg5g5kElb8IYSF9xrCKAqKD15PPO68x/PxgOZiT2gMihVKW1DcQp3w+KLYFiLR0KhCriOkN5TPqEJU8C6j1yCMIhH3Hx2RiaZNt40yTruiNBZDiKbA4w8bFGDalBBk5kQwIsRoCoYkCnAuoX3ob5ni/3F2EmJF3yIkDCOzYe+mNnFf58FvkCv32HmXPlHoNZEsmHiLroXxGzz+hD7h5Q9CCDZhQeifQfnn9z1Yxx9ORGlgJAQ2ub4bjHaxTC0hZ+TNdxDWGMfkt6hImfM+5GW6MSmDIZbvvvsuaZUFSMAEXn36CWUeXnQuhmXfeust4/cOooLlH5ZlI8aOBwieFIkIiIAIiIAI5EMAxYuY40yx6+SBh5MQkEyCEcAEV5TkfJUXQlwoOxoLzzuPUBY5szKRrp3jqGR45xmVIZwrCL9inWnickhT6CchL0wAZ5Rh968Osy8nfldoFlnTP7fufTlDgEIGIaoCozSE5YRzmT7p99xDpu9Hru8G18IaAzuMrGUqqz4ez3sEgGE7hhbTPRwOOOAAb0XVF0DESGJB82MSxMnFlX/uEy9J8JQwfCsRAREQAREQgUII4PHMpvyTVyblJpSD4lKo8pIuvCnbDy+FsvRZ8wTQsdKNeBRb+edOMBjznVtQ03dOHy40lC/6i8fp6pfru8H95/MjeOnyrg/H8p4DQIfMtMQax7Ha6oOwviyz6Blq44dL8vlVx/pw37oHERABERABERABEQgEmlU4pbxpdytz/7LJ+u1T50hmS6tzpUMgbwMALz+TZ4gbY8goDJ2wTBde8nyXECudW69aE4bemPBDXB9xgywLJREBERABERABERCBRY1As8qm9vZGz1ibhq2sUXlkeSAHokl5Y+vUuIN9O+AtW6/DWosamnpxv3mHADFxh19dY2ISkycYpkJhZv1YfrCBCcJ1XZgMguLP5BNWZIj+CBojIMxWl4iACIiACIiACIjAokCgUUVD+27zd+yxv561h4c+bcOmjbBWDVragM4b2gE997B2jdosChjq5T3mbQBw96wZvMcee/iZ84TIEPbDBOBSiSFb0BYKy4CxvBR/UWFOgAyAKBFti4AIiIAIiIAI1HcC5WXlttti2/u/+n6vi9L9ZTUAWJOV9YSjM7JR+utLvH+8ocNyYfHj2hcBERABERABERABERCB+kIgqwHAWv9MguVX44LwK2ysSlCffgE43Ftd+uSHK1jiKiqsj5tuxaJoGm2LgAiIgAiIgAiIgAgs2gTyngQcMDEXgB/BkixcAvz0NT9EEv3jmEQEREAEREAEREAEREAEshEo2ADIlpnOiYAIiIAIiIAIiIAIiIAIlDYBGQCl3T6qnQiIgAiIgAiIgAiIgAgUlYAMgKLiVGYiIAIiIAIiIAIiIAIiUNoEsk4CLu2qL9q142fb+Rn5qDRq1Ci6q20REAEREAEREAEREAERqEIgVYOsctrsvvvus9deey15ZuTIkXbFFVdUmQjMOvmXX355Mp02apbAsssuW7MFKHcREAEREAEREAEREIF6SSCrAbDBBhsYv4AblT59+kR3k9taFjSJQhsiIAIiIAIiIAIiIAIiULIEshoAl1xySclWXBUTAREQAREQAREQAREQAREonIAmARfOTFeIgAiIgAiIgAiIgAiIQJ0lIAOgzjadKi4CIiACIiACIiACIiAChROQAVA4M10hAiIgAiIgAiIgAiIgAnWWgAyAOtt0qrgIiIAIiIAIiIAIiIAIFE5ABkDhzHSFCIiACIiACIiACIiACNRZAjIA6mzTqeIiIAIiIAIiIAIiIAIiUDgBGQCFM9MVIiACIiACIiACIiACIlBnCcgAqLNNp4qLgAiIgAiIgAiIgAiIQOEEZAAUzkxXiIAIiIAIiIAIiIAIiECdJSADoM42nSouAiIgAiIgAiIgAiIgAoUTkAFQODNdIQIiIAIiIAIiIAIiIAJ1loAMgDrbdKq4CIiACIiACIiACIiACBROoLLwS3SFCIiACIiACIiACIhAfSHQskFLa9uwdcrtxPdTTkZ2mlQ0rnItpyvLpWJGMJXcplqn5JokvwrNmDHDEolESuKysjJr3LhxyjHtiIAIiIAIiIAIiEA2Ag+vdUu201nPHdJrb+NPUrcIyACoW+2VrO23335rU6dOTe6z0aRJE+vfv3/KMe2IgAiIgAiIgAiIgAiIQJSA5gBEaWhbBERABERABERABERABOo5ARkA9byBdXsiIAIiIAIiIAIiIAIiECUgAyBKQ9siIAIiIAIiIAIiIAIiUM8JyACo5w2s2xMBERABERABERABERCBKAFNAo7SqEPbXbp0sVmzZqXUuEGDBin72hEBERABERCBYhBI/P23zXrmySpZVSy7vFX2W7XKcR0QgUIJzP1rqM15+80ql1WuuY5VLLlkleM6sGAEZAAsGL+FdnX37t0XWtkqWAREQAREYNEikHCrzs1+5eWqN11RKQOgKhUdqQaBxOjRaftY+WKLywCoBs9clygEKBchnRcBERABERABERABERCBekRABkA9akzdigiIgAiIgAiIgAiIgAjkIiADIBchnRcBERABERABERABERCBekRABkA9akzdigiIgAiIgAiIgAiIgAjkIiADIBchnRcBERABERABERABERCBekRABkA9akzdigiIgAiIgAiIgAiIgAjkIiADIBchnRcBERABERABERABERCBekRABkA9akzdigiIgAiIgAiIgAiIgAjkIiADIBchnRcBERABERABERABERCBekRABkA9akzdigiIgAiIgAiIgAiIgAjkIiADIBchnRcBERABERABERABERCBekRABkA9akzdigiIgAiIgAiIgAiIgAjkIiADIBchnRcBERABERABERABERCBekRABkA9akzdigiIgAiIgAiIgAiIgAjkIiADIBchnRcBERABERABERABERCBekSgsh7dyyJ1K2PHjrU5c+ak3HNFRYV16NAh5Zh2REAEREAEREAEREAERCBKQAZAlEYd2v7jjz9s6tSpKTVu0qSJDIAUItoRAREQgdonkEgkrKysrPYLVok5CahtciJSgkWEgAyARaShdZsiIAKLJoGDDjrIRo0aZYcffrhtt912VSBcf/319sILL9g+++xje+65pz//+uuv24MPPmjDhw+3jh072k477ZT22iqZFfnA9OnT7c4777R33nnHpkyZYn369LEjjzzSllpqqbxKeuihh+y5556z++67LyV9vvc3bdo0z+2www6z9dZbLyWPdDt33XWXPfnkkzZr1izbcccdjeuyyS+//GInnHCCT3LPPfdYu3btUpKjrO699942adIku+mmm2yxxRbz5z///HN79tln7auvvrJll13Wl7PEEkukXFsbO7PmzbOXvv7GXnvrHZs4caKttdZavn0aNmyYV/Hp2mf27Nl2++2325tvvmmNGjWy9ddf3zPAwYXAa+bMmWnzv+SSS6xFixZpz40ePdpuueUWe+mll3z/OeSQQ2zttddOmzYcvOOOO+zxxx+35ZZbzi699NJwOPn5zTff2L///W9bfPHF7YYbbkgeDxvXXHONTZ482c4444xwqGiff//9tz388MP2xhtveE7bbrut/55mKiAfbkQV8F2BEX1/xRVXtOOOO84/A0K+pdL3Qn30WX0CmgNQfXa6UgREQARKnsCIESNs2LBh9swzz1Sp6zynwD3xxBP2119/eUWFBCilp5xyiqFQotz07dvXzj77bG8QVMmghg+gUFO/zTff3Ct+M2bMsP3228+GDh2as2Tu47LLLvNGTDRxvveHwXHqqacaSh4KfS755JNPvBJ41llneYUPhfCLL77Iehn5wp72ee2116qk5frvv//epwkhn4z+HnroodayZUvfTijM+++/v9HOtS0PjRlnN73zrm2yySa+XVBGTzzxxLyqkal9zjzzTLv77ru9co5B+uqrr9p///tfwxhC6Jfxv59//tl+/fVXy2Z4oPz/9ttvXsFdZplljHYKTDNVGKMmtM24ceOqJHv++ed922Bgx+WBBx7whgyKek3IxRdf7I2T3Xbbzbbccktj/5FHHslYVJwZ+3FuN998szc0t9hiCzv22GM9U4zY0P9Lqe9lvFGdyJuARgDyRqWEIiACIlA3Cay22mr22WefGUpM1MvMsfLycmvfvn3yxvBEk/7000/3x/B8o+DgrQ0jBMnEGTZQ1BlV4Jr777/fGjRokCFl5sPkwfUoIrvssotPiMd200039d7vo446KuPFKCwYLz179rTx48enpMvn/j744AOvIGbyJqdk+M8Oyi/l9evXzx/p1KmTff3118n9dNeEY/B+5ZVXbPfddw+H/CeeWLzPP/zwQ/I4IwGkP+mkk/yxVVdd1SvglM9oQT5SjPaZOneuPTB6jB0zYDPbddddfbF47KnXmDFjsoajZmqfwYMH+36DEXHAAQf4PFdZZRVvAMICRff4449PucWPPvrIULZRXik/k8Bnr7328m1EPvTLkSNHWvfu3TNd4o8z6gIvDLRo+8x190+b0T5BQeYCRhowWGj7Ll26ZM2bk4xw/fnnn/67tfTSS+dMTwLyZgQI43jJJZf012BoMHKy8847G/MB45IPNwya7bffPsmePrzvvvvad9995/txMfpevF7aX3gEZAAsPPYqWQREQARqhcBKK63klYy4EoNShXf95ZdfTtZjm222sbZt2yb32WjdunWKkpNyMrKDNxsvJGEweGMJHUIZQSG8+uqrIylTN8855xxviESP4tnGS7vGGmskD6PgEQqCQpZNCL1gQYR1113XK0XRtPncH8YLIRWEiRDWko+ss846fpSEkRaUwAkTJthmm22Wz6W+Dc4///wUAw3vNN5vQriiBsAFF1yQcv+wJVSqefPmOcvK1D6E2xAWlU4IMUpnWDRz7frkcstYUxcmEoSRGdo7myJO2kztg4ceGTBggP/kv86dOxse+w8//NAbAMkTboNRGvoI4Wv9+/ePnqqyjfFIeAt9gr6IYZFL+ScT5nLQjnED7dNPP/V9fOWVVzZGf4IwasP9E55z1VVXhcMZP6k3YTUYF+S1xx57eIMum9GMQt6jR4+k8k/mG264oef6+++/+1C5jAW6E5m48Z1hgZEghC8hbdq08Z8L0vd8BvqvpAjIAMjSHFj4/GUbVsxyeY2eIgY2PnyZzuqv0UoocxEQgTpBACUGRT+qxPD8wCC48cYbUwyAuCKFsoBCjNc0nRCagXLGnIH33nvPe6cJ40AhCUoM5VdWFva6wfu+8cYbpxSJooqCki0e/+OPP/be0UcffTRtWE0+90fIE89Tnv/5CnVidALvb9euXe3ee+/NS8Ekfww05lpEDTSUyqZNm/o47GgdqFezZs38yAbGBnMcVl99dV92NF3Yzqd9GAXK1D7Z3itNKtx17loMlBdffNGeeuopP2JDeFImydY+4ToMCRgieNeHDBmSMnIV8ibMin5MnHouIZyL/oOiTTude+65uS5JnickhlGG6AhaMJ7j72GMBdLnK4wgXHfddT7U6LHHHvOhPMw3YNSLkZXo6FzIk7k58eNhn+8Hc2WySSZujLYxckYIIMYRfYuQO8KFkEL7XrY66NzCJ1DYE3nh17fWakBsLB4gHsrEoZaatGrVqtSqpPqIgAiUMAG8qnhAgxJD6AQK1/LLL5+x1niWCR3Au5xJycJry8RcwheYMNmrV68q+aGcDBw4sMrxQg78+OOPPg9GFVB400mYcEn4Es/uXJLp/rIpvenyRMkmtOjdd9/1HmuUwqCQocDmciKlM9BQMFEkOZdOULrx2uPxxUsfV0TDNfm0zwYbbGD8VVeoK8Yfozb0FXikq3eu9qEvwu3WW2/1HNlmIi4TUmmrqDAK9PTTT/vRiVx8mUSNYcboEROISY9xlU/bUCbzYBiJCAYa98k29SQUJyopfcdxSLjQnNmvv2oJ1yfKu3W3iqWWtrI0k6S7devmv2uEtmFMX3nllUafTzdyRuhSXAcI4WqMPGWTbNwY+eN7g/FJv8L4Z/QlLvn2vfh12i8tAjIA0rQHDwgsYTwaTK6SiIAIiEBdJ8CKHsQkByUmKJiZ7gtDgecgCgWKTlAw4unx1B7g4rWJRz7mmGO813KHHXZIhg2QniWL33///filyX28pukUxpAAY4XYcsI3wtyEcC76iVeX+PhoCEn0fHQ73/uLXpNpm0mrTDJFWWNFGOZK4HE++uij/YRdFLpcKxcxQsOoAR5clDti1okPR/lNJ7DgjxCgAw880Bh1Sacs5tM+KHtMok0nKISEpmQT2oY/DCD6AMr11ltvXeWSXO3DdRdddJFnx4pVjRs39qMjjD7Fl71mNAujIMwPqVJY5MARRxzh+xdzSuiHsEKpZ4IvfQvDNZvQN+lTYQSNOSIoyxhecQMg5JOY8rfN/XGQlc2eZTMfcG2YmGfuhsycc7Hp2edZeafOIWnykzk5YWUfRoWicw6SidwGRhZGQFSCgRTCdaLnotuZuGHUEEqF0Um/w0iCDauHMTpE/wySb98L6fVZmgRkAMTahVg8Jr2wokI+HqTY5doVAREQgZIlgBJDvD9LVKJgonCmE55/rP5BqAlpgjc7XVqUNiZtMgpAqBChEiwtiiLBCiUYHijbF154YbrL/TFWkUnxnEZSorAQloBSffLJJ2c0FIhvJy2eVCZCIqziwh/7GDOEJSGF3J+/IMd/eKJRWMN8AVYfQnGivhgEuZR/sl9hhRV82AtefSZf8v4hlIMJn3FhhJqwHYS4bdoTAwQlLoRdhWvyaR+UYjzt6QRlL5MBMA8Pt/PEz5s43+u8Tt8VbBlX53fcPWy5TuoSm8Ncn/Lt44zQnXfcwRc1ceIkm+i88+wfffAhtuG669iqfXrbs/ffZ4PcKk4d3LKoPVwoypEnn2KLuXYN5XDx426uyUaubu0bNkg5Hr+H3wYP9iFKd1x7jbWtrLBtN1jfBu28k13u2ogRgb1cqE003+j1CZYbdaw5P8DdD6M8o3/71V50oVdbbLSRP06ahAsVi+bBsemnnmwJjDdGcObMnp/tP7/dM+3//mWNjj/JKtyk8akuzXMvv2KPuPCpv1xoz5abbmL33XSjLf1PGE8031C39i487ic3AhQ9N+Efg6BLi+Ypx8M14TMTt08+/cwbWfs7NpXTpto897fGMkvbKiv2tTfcM2Oz/mv4LKJ9r12DStthyy3s1rvvsZljRlfpe6HMfD8TU6fkm1TpikBABkAM4m233eZfIMyGZ8UAiQiIgAjUFwIo5Xir8VriGQ4riETvD+WYkU+UVtZVR4HMR/DWEp7DH55MvN7kQzgBExaJvy5UGIVlDXViknOtQMREZcI8osJETbzSXNvTKVtIde/PX5zhP5RxwkyCEKJE7DSe1HwnAnMtXlYm/qLU01bpBM86a+OfdtppydMYP3hpM8XxkzBb++D04q8Q+W7qNDv61z/sWve54puv+0tnO2V55B+/W+9xY23a8cekZNfQKcmndp8f12+znGLt5PN5c+1DZ0Ts5PY733uXTXz4frt++Ejbob3zruMtdzJyzlz74vsfbZuxo5J5znTlfP3tD3Z6j27JYz5xmv9mzZhf1qgrLrNprebPTTjClfmeMwaGuhCXvu+9bdO++izNlU5vd3VJTJrsy+jpUnR1q1k9e8zR9vbI0Xb7Ukv66+YMG2Hz/p6SoR4sXZomhMvVf+aVl/ky7xgxyp4fP9F2dPe83VK9rNXwoWbXXW3px338JbaEM5zu+vMvG3n0kdbS3Qfy4fgJ1sAZG60uvsCm/WMczk/9v/+zcZvt7gGZfu5AmxZZTWniz79YWzcHg/bc7Yefbe2WLeyE7l2Smf45dJhVOMNz1skn2GyMHUmdISADINZUAwcO9N6X2GHtioAIiECdJ0DIApP7iC8mbCSdsBoNYZB4zaPr2KPk5vrhpJAfYTj8EaaQybMf0mb6JBTmvPPO8151lmIkxjwI3vHgVWdpQmLHmYQbDwchxhsjIHq8GPcX6hE+8cATfkM9CFfB6GE1JJZcJaSjd+/efjQkpM/0iQHAnDNWkcm0zOmGbhSDyaJ84plnJAdDidGWbGFU0TKL0T5LNWlsnZxCfJNTYP/jFPEmrn/cO2qsTXYK+zbt5q8aM8Qp3y9NmGjbuf3OLqRke6fkRmWWU8S/nDI15fiY2XPsxuGjvHKPOnmhUzBXad7MNvxHeef6IXjd3Wevf4wEjmWSxRs3shWcEXuzy7OhU1CXcNc8Mmas/TVzlrV2E6rPHDzUru3d03rmkdcmrVvZHaNGW/dGDa2Hy7cYsm27tnZA545WUYDyvJ5j0cF53zGWTnLK+AQ3v+CeUWNs/04drIFrByTO3h/Lwq1vs6bWzbXRJUOH21mLd7dGLp+nxo63P1wbHtVlfrjSuq1a+GN8kv4dZxy9OmGSN17y7Xu+cvqvJAjIAIg1A0Ov+QgxsSxlhjD5Sp0/H2pKIwIisLAJoGQy0pnOw8zqInjMkfALtaG+eJcJkSxEmDhZXWEFEiYhUp9Qp5AXXnCWJERQmFHAs60MFK4r9v2FfFmthbXcr7jiCv+LsYROMYJMGBAx7cyPyLQ+e8iDTyZcYuwQ4x1+9Td6nm0mZRMPT4hW8PhTFiFOhcqCtE9DpyBe3KuHnes80bsP+sXwQ7d2feTcnot55ZC6DHMG2N1OMV2rZXNvAORTv8O7dLKrnVd9h+9/8krxai2a2Rk9uqe8Y/90SinGQc88lfAzFu9mZ7t6nvz7n74KXV3Y0BVL9rQlnKF1wm+D7W2nyOZjAGzcpqXdM3qMbeoMgWJJR1eXQgVj4YKePewMZ7wM+HaQNXL7G7Ruafs6AyBIOvbZuKHwX+La84Ihw2w72LuMWOWJUZu1ncKPHOHaZurceXa8YzZ/3MFs1w7t7DBnwEjqHoEyN1sfQ1qShgDxnKxswQsmLsRlMsyNEC7ENkuVSURABERABBZNAow48BsAhFeFGH1I8CNNmSZRV5cUk4MpizkP8bj/6uaZ7bp5bjRn2mknp00yzoWAzHaqBCMCxXKGMZLQqLzMe6LTFlqNg5Ocs26Oq2c7V88g/KBZU6f8FqveId/a+hw9a7a1cWFAwfNfjHInO05TnKLf2Rkn5WlGJqY5ZmPdSE0Xd76Y5Waqe6ODD7MG62+Q6bSOV5OARgCqCY4Z+vwh/Jx2MAaqmZ0uEwEREAERqOMEWDkl3Y9LFVv5BxNzM3r27FkSxKIKdbEqFGLbi5Uf+bRyIxRx4QfN6rJUZwQh1/22dJxaVkWVvKypY9ajjnNL3swivJGliRdhKrp1ERABERABERCBnATKFuthFb375Ey3KCWY50Zm5v3wnVv6M3OARVm3+b8JsChxyXWvibFjbO633+RKpvNFIiADoEgglY0IiIAIiIAILGoEKlfoa4320Ip58Xaf9dILNuvB++OHzRo1tvLFe1rT01NXrKqacNE7MueLz2UA1GKzywCoRdgqSgREQAREQAREoP4TaLjFVv6Xf2c9/6zNG+rmB7q4+fKOnaxyo42t4Uab1H8AusOSJyADoOSbSBUUAREQAREQARGoawQq+65o/ElEoBQJyADI0ioffvhhlrM6JQIiIAIiIAIiIAIiIAJ1j8D8X4yoe/VWjUVABERABERABERABERABKpBQAZANaDpEhEQAREQAREQAREQARGoqwRkANTVllO9RUAEREAEREAEREAERKAaBDQHoBrQSuGSP/74w2bOnJlSFX6EplevXinHtCMCIiACIiACIiACIiACUQIyAKI06tD22LFjberUqSk1btKkiQyAFCLaEQEREAEREAEREAERiBNQCFCciPZFQAREQAREQAREQAREoB4TkAFQjxtXtyYCIiACIiACIiACIiACcQIyAOJEtC8CIiACIiACIiACIiAC9ZiADIB63Li6NREQAREQAREQAREQARGIE9Ak4DiROrLfr18/SyQSKbUtKytL2deOCIiACIiACIiACIiACMQJyACIE6kj+xUVFXWkpqqmCIiACIiACIiACIhAKRFQCFAptYbqIgIiIAIiIAIiIAIiIAI1TEAGQA0DVvYiIAIiIAIiIAIiIAIiUEoEZACUUmuoLiIgAiIgAiIgAiIgAiJQwwRkANQwYGUvAiIgAiIgAiIgAiIgAqVEQAZAKbWG6iICIiACIiACIiACIiACNUxABkANA1b2IiACIiACIiACIiACIlBKBGQAlFJrqC4iIAIiIAIiIAIiIAIiUMMEZADUMGBlLwIiIAIiIAIiIAIiIAKlREAGQCm1huoiAiIgAiIgAiIgAiIgAjVMQAZADQNW9iIgAiIgAiIgAiIgAiJQSgRkAJRSa6guIiACIiACIiACIiACIlDDBCprOH9lLwIiIAIiIAIiUMcJlHXsaM1uurXqXVQ2qHpMR0SgGgQqVlo5fR9r0LAauemSXARkAOQipPMiIAIiIAIisIgTKCt3AQNNmi7iFHT7NUmgrKJCfawmAcfyVghQDIh2RUAEREAEREAEREAERKA+E5ABUJ9bV/cmAiIgAiIgAiIgAiIgAjECCgGKAakru99++61NmzYtpbqNGze2lVZaKeWYdkRABERABERABERABEQgSkAGQJRGHdqeMWOGTZ8+vQ7VWFUVAREQAREQAREQAREoBQIKASqFVlAdREAEREAEREAEREAERKCWCMgAqCXQKkYEREAEREAEREAEREAESoGADIBSaAXVQQREQAREQAREQAREQARqiYDmABQBdOfOnW3q1KnWo0ePIuSWXxazZ8/2CcvKyvxnIpHwnw0a6EdZ8iOoVHWZAP0+9Pm6fB+quwjkS0B9Pl9SSldfCKjPL1hLDhgwwG677baMmcgAyIgm/xMnnniiTzxy5Mj8LypSyo8++sjmzp1r66yzTpFyVDYiUNoEvvvuOxs2bJhtvvnmpV1R1U4EikTgzz//tC+//NK23XZbq+DHkiQiUM8JjBs3zt59913bcMMNrXXr1vX8bmvm9pZbbrmsGZc5L9p813HWZDpZqgSOPPJIY0WgO++8s1SrqHqJQFEJXHLJJfb888/b22+/XdR8lZkIlCqBxx57zE4//XT7+uuvjeWeJSJQ3wl88cUXtueee9qTTz5puRTZ+s6ipu5PcwBqiqzyFQEREAEREAEREAEREIESJKAQoBJslEKqtP322/sQoEKuUVoRqMsE1ltvPevatWtdvgXVXQQKIrDCCisYoaaVlXplFwROiessgW7duvk+37Fjxzp7D6VecYUAlXoLqX4iIAIiIAIiIAIiIAIiUEQCCgEqIsyFkZWmcCwM6ipzYRNg4vusWbMWdjVUvgjUGoF58+bVWlkqSARKgYD0m5ptBRkANcu3xnJnRYi9997b2rRpY7169bJzzz23xspSxiJQSgRQhFgN5bDDDiulaqkuIlB0Aj/88INttdVW1rJlS2vatKmtttpq9uqrrxa9HGUoAqVCgCXOr7zySltzzTWtefPmtsEGG9jHH39cKtWrV/WQAVAHm3PatGm2yy67+Jq/9dZbdtZZZ9lFF10kI6AOtqWqXBiBmTNn2hFHHGEvvvhiYRcqtQjUMQLjx4+3zTbbzMaMGWO33HKLPffcc37uCwbB559/XsfuRtUVgfwI0NfPPvtsO+CAA+z111835gJsvPHGNmjQoPwyUKq8CWhGUd6oSifhxRdfbGPHjrXbb7/dLwm38sor25AhQ+yaa66xU0891Ro1alQ6lVVNRKBIBFB69t13XxsxYoRpYliRoCqbkiXw1FNP2fDhw43P1Vdf3dezf//+3gi46667bNVVVy3ZuqtiIlAdAtOnT7fzzjvPDj/8cO/oIY/ll1/ennnmGXviiSf8UrjVyVfXpCegEYD0XEr66CuvvGJbbLFFynrQ2223nTcKPv3005KuuyonAtUlwC8a4g1ifWjC3iQiUJ8JrLLKKnbDDTcklX/utUWLFta+fXubOHFifb513dsiSqBJkyb2zTff2BlnnJEkMGHCBGPkt2HDhslj2igOAY0AFIdjreby66+/2tprr51SJooRsjB+jTilItoRgRoiMHDgQOvUqVMN5a5sRaC0CGAA8BeVDz74wAYPHmynnHJK9LC2RaDeEOjQoYO/FxZ6IPb/ggsusHbt2iXDnuvNjZbAjcgAKIFGKLQKkydPtrZt26ZcFn4qe/To0SnHtSMC9YWAlP/60pK6j+oQmDp1qh199NHWp08fO+SQQ6qTha4RgTpD4PLLL7fTTjvN15eQtyWWWKLO1L2uVFQGQF1pqUg9+TGYioqKyBGzsrIy/8dQmUQEREAERKD+EJgyZYoR5vnbb78ZCz9onlf9adv/b+9uQuHr4gCO/56k1CQNKUJNUURJCknK64jsRCIWLJQdUhZWygoRWysjrxs2QmoWprxsbFiIssBCbCYWFvQ8/9/tMRnmb5gxL67vral7zz333Hs+Zxb3d++551AT3wItLS1SXV0tDofDGPFNeze8BAS+jyD1qwIEAF8Vi4L8KSkpov3iXi/aJ1THzNXh4lgQQAABBMwh4Ha7pa6uTk5PT2Vra0sKCwvNUTFqgcAHAjabTfSnH7tfXFzIzMwMAcAHXoHs4iPgQNQifExqaqrc3Nx4XcXLNq/JvFjYQAABBH6sgHb3tNvtRr9/p9MppaWlP7YuXDgC/gT0TZf+z+/v772y6qAn19fXcnl56ZXORnACBADB+UXk6MrKSuNJ0OuZIXVcdIvFwtBwEWkRTooAAgh8r4C+0W1sbJSrqyvZ3d0VHe6ZBQEzC+hw5jrmv84F8HrRN19xcXHGELiv01kPToAAIDi/iBytEyHpPADaH04j5v39fRkbGzMmz0hISIjINXFSBBBAAIHvE9APH/XGv6mpSXR454WFBc9PvwNgQcBsArm5uVJWViZTU1Oys7Nj3OeMj4/LxsaG9PX1vfv20Wz1D3d9/vnzlOHfcJ+U8wUvoBNjdHV1ic4WabVajSGydMzotx8HB38mSkAg+gS0K0R2drboTRILAmYUKC8vF5fL5bNq+k3A5uamz30kIvCTBXTyu+7ubuP/rYObxMbGysDAgPGAU9dZvk+AAOD7LMNeksZuOiZ0RkaG6MhALAgggAACCCCAwE8X0Iebt7e3xqSP3PiHpjUJAELjSqkIIIAAAggggAACCESlAN8ARGWzcFEIIIAAAggggAACCIRGgAAgNK6UigACCCCAAAIIIIBAVAoQAERls3BRCCCAAAIIIIAAAgiERoAAIDSulIoAAggggAACCCCAQFQKEABEZbNwUQgggAACCCCAAAIIhEaAACA0rpSKAAIIIPBH4OzsTHQ878zMTHk77YxO8KP7PvqVlJQYjrW1tR/mGxwcxBsBBBBA4JMCDB7/SSiyIYAAAgh8XcDhcEh+fr4cHx+L0+mUqqoqTyEFBQXG7LYvCSsrK7K2tuaVlpSU9LJbsrKyZGRkxLP9eiUnJ+f1JusIIIAAAh8IMA/ABzjsQgABBBAIXECf+OuT/56eHmNmz9TUVK+b+7clDw8Py+jo6Ls3BZpP3wA8PDzI3t7e28PYRgABBBD4ogBvAL4IRnYEEEAAgc8JuFwuubi4kPr6eomPj5f+/n7RGT4TExM/VwC5EEAAAQRCIsA3ACFhpVAEEEAAgbm5OdGuOdoFqLm5WZ6fn2V+fh4YBBBAAIEICxAARLgBOD0CCCBgRoHHx0dZXV2VtrY2o3rJyclSU1Mjs7OzAVf34OBA4uLifP4uLy8DLpcDEUAAgd8mQBeg39bi1BcBBBAIg8D6+rq43W5pbW31nE2Dgc7OTjk8PJTi4mJP+mdX0tPTpbe312f2hIQEn+kkIoAAAgi8FyAAeG9CCgIIIIBAkALa/UeXoqIiT0naBUgXfQsQSACQlpYmQ0NDnvJYQQABBBAITIAAIDA3jkIAAQQQ+IvAzc2NbG9vS0dHh1RUVHjl0m8AlpaWZHJyUiwWi9c+NhBAAAEEwiNAABAeZ86CAAII/BqBxcVFeXp6Msbst9lsXvXWoUAbGhpkeXlZurq6vPaxgQACCCAQHgECgPA4cxYEEEDg1who95/S0lJ5e/OvAHa7XVJSUoxuQF8NAO7u7v46j4DVajWGG/01yFQUAQQQCEKAACAIPA5FAAEEEPAW0Bl/j46OZHp62nvH/1sxMTHS3t4uExMTcnJyInl5eT7z+Uo8Pz83jvW1T2cV1vkGWBBAAAEE/AswE7B/I3IggAACCCCAAAIIIGAaAeYBME1TUhEEEEAAAQQQQAABBPwLEAD4NyIHAggggAACCCCAAAKmESAAME1TUhEEEEAAAQQQQAABBPwLEAD4NyIHAggggAACCCCAAAKmESAAME1TUhEEEEAAAQQQQAABBPwLEAD4NyIHAggggAACCCCAAAKmESAAME1TUhEEEEAAAQQQQAABBPwLEAD4NyIHAggggAACCCCAAAKmESAAME1TUhEEEEAAAQQQQAABBPwL/AceSWO4/SLOCAAAAABJRU5ErkJggg==" /><!-- --></p>
<p>This plot shows the ATE specific for each fold and for the weighted-mean results over the fold with corresponding pooled variance. The rule is the pooled rule which includes all observations that were indicated by the fold specific rules.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pooled_marginal_results <span class="ot">&lt;-</span> sim_results<span class="sc">$</span><span class="st">`</span><span class="at">Pooled TMLE Marginal Results</span><span class="st">`</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>pooled_marginal_results <span class="sc">%&gt;%</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(<span class="at">caption =</span> <span class="st">&quot;Pooled Marginal Results&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="fu">kable_classic</span>(<span class="at">full_width =</span> F, <span class="at">html_font =</span> <span class="st">&quot;Cambria&quot;</span>)</span></code></pre></div>
<table class=" lightable-classic" style="font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Pooled Marginal Results
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Marginal ATE
</th>
<th style="text-align:right;">
Standard Error
</th>
<th style="text-align:right;">
Lower CI
</th>
<th style="text-align:right;">
Upper CI
</th>
<th style="text-align:right;">
P-value
</th>
<th style="text-align:right;">
P-value Adj
</th>
<th style="text-align:right;">
RMSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
M1_2-M1_1
</td>
<td style="text-align:right;">
0.330
</td>
<td style="text-align:right;">
0.089
</td>
<td style="text-align:right;">
0.155
</td>
<td style="text-align:right;">
0.505
</td>
<td style="text-align:right;">
0.000227
</td>
<td style="text-align:right;">
6.8e-04
</td>
<td style="text-align:right;">
0.620
</td>
</tr>
<tr>
<td style="text-align:left;">
M2_2-M2_1
</td>
<td style="text-align:right;">
-0.563
</td>
<td style="text-align:right;">
0.107
</td>
<td style="text-align:right;">
-0.774
</td>
<td style="text-align:right;">
-0.353
</td>
<td style="text-align:right;">
0.000000
</td>
<td style="text-align:right;">
0.0e+00
</td>
<td style="text-align:right;">
0.819
</td>
</tr>
<tr>
<td style="text-align:left;">
M3_2-M3_1
</td>
<td style="text-align:right;">
-0.528
</td>
<td style="text-align:right;">
0.118
</td>
<td style="text-align:right;">
-0.759
</td>
<td style="text-align:right;">
-0.297
</td>
<td style="text-align:right;">
0.000007
</td>
<td style="text-align:right;">
2.2e-05
</td>
<td style="text-align:right;">
0.900
</td>
</tr>
</tbody>
</table>
<p>This plot shows the data-adaptively identified quantile comparisons for each variable. Here <code>M1_2 - M1_1</code> shows the second quantile for variable <span class="math inline">\(M1\)</span> minus the first quantile for <span class="math inline">\(M1\)</span>. As expected, this difference is positive and the other two are negative given how we simulated our data.</p>
<p>Similarly we can investigate and plot the v-fold specific estimates:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>marginal_plots <span class="ot">&lt;-</span> <span class="fu">plot_marginal_results</span>(<span class="at">v_marginal_results =</span>  sim_results<span class="sc">$</span><span class="st">`</span><span class="at">V-Specific Marg Results</span><span class="st">`</span>, <span class="at">mix_comps =</span> <span class="fu">c</span>(<span class="fu">paste</span>(<span class="st">&quot;M&quot;</span>, <span class="fu">seq</span>(<span class="dv">3</span>), <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)),<span class="at">hjust =</span> <span class="fl">0.8</span> )</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>marginal_plots<span class="sc">$</span>M2</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --> Same as the mixtures plot, the marginal plot shows the ATE for an individual variable in the mixture with corresponding ATE and variance estimates. The top rule is the pooled rule for the reference category, the rule(s) in the boxes are the pooled rules for each quantile that was found for the variable of interest.</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-coyle2021sl3" class="csl-entry">
Coyle, Jeremy R, Nima S Hejazi, Ivana Malenica, Rachael V Phillips, and Oleg Sofrygin. 2021. <em><span class="nocase">sl3</span>: Modern Pipelines for Machine Learning and <span>Super Learning</span></em>. <a href="https://doi.org/10.5281/zenodo.1342293">https://doi.org/10.5281/zenodo.1342293</a>.
</div>
<div id="ref-Fokkema2020a" class="csl-entry">
Fokkema, Marjolein. 2020. <span>“<span class="nocase">Fitting prediction rule ensembles with R package pre</span>.”</span> <em>Journal of Statistical Software</em> 92 (12). <a href="https://doi.org/10.18637/jss.v092.i12">https://doi.org/10.18637/jss.v092.i12</a>.
</div>
<div id="ref-Hubbard2016" class="csl-entry">
Hubbard, Alan E., Sara Kherad-Pajouh, and Mark J. Van Der Laan. 2016. <span>“<span class="nocase">Statistical Inference for Data Adaptive Target Parameters</span>.”</span> <em>International Journal of Biostatistics</em> 12 (1): 3–19. <a href="https://doi.org/10.1515/ijb-2015-0013">https://doi.org/10.1515/ijb-2015-0013</a>.
</div>
<div id="ref-vdl2022targeted" class="csl-entry">
van der Laan, Mark J, Jeremy R Coyle, Nima S Hejazi, Ivana Malenica, Rachael V Phillips, and Alan E Hubbard. 2022. <em><span class="nocase">Targeted Learning in <code>R</code>: A Causal Data Science Handbook</span></em>. CRC Press. <a href="https://tlverse.org/tlverse-handbook/">https://tlverse.org/tlverse-handbook/</a>.
</div>
<div id="ref-vdl2011targeted" class="csl-entry">
van der Laan, Mark J, and Sherri Rose. 2011. <em>Targeted Learning: Causal Inference for Observational and Experimental Data</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-vdl2018targeted" class="csl-entry">
———. 2018. <em>Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies</em>. Springer Science &amp; Business Media.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
